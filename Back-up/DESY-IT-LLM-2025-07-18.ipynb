{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f06297-9915-4acc-b400-f1847cb3881e",
   "metadata": {},
   "source": [
    "### 1- Scrape and Extract Website Data\n",
    "web scraper with GPU detection, JavaScript rendering (using Pyppeteer), and content extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a20c991-a7c9-4415-9082-79360e0c89f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c1fb2a5-d356-4f5c-98b2-ec559b2d3115",
   "metadata": {},
   "source": [
    "# DESYContentProcessor (Part 2): \n",
    "## Processes and extracts content from the mapped URLs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e735f0-8b4f-4244-8f4c-fbc5206f3cdb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6034ef97-86d5-4b10-bb34-4f0b2e2012ad",
   "metadata": {},
   "source": [
    "## 📊 Comparison of Scraping Packages for Mixed HTTPS and JavaScript Pages (Async, with Wait Functions)\n",
    "\n",
    "| Feature | `requests` + `BeautifulSoup` | `aiohttp` + `BeautifulSoup` | `httpx` + `selectolax` | `Scrapy` | `Playwright (Python)` | `Pyppeteer` |\n",
    "|---------|------------------------------|------------------------------|------------------------|----------|------------------------|-------------|\n",
    "| 🏗️ Created by | Community (Python) | Community (Python) | Community (Python) | Community (Python) | Microsoft | Community (unofficial, port of Puppeteer) |\n",
    "| 🛠️ Language | Python | Python | Python | Python | Python | Python |\n",
    "| 🌐 Browser Support | N/A (HTTP/HTTPS) | N/A (HTTP/HTTPS) | N/A (HTTP/HTTPS) | N/A (HTTP/HTTPS) | **Chromium, Firefox, WebKit** (Safari) | Only **Chromium** |\n",
    "| 📦 Install | `pip install requests beautifulsoup4` | `pip install aiohttp beautifulsoup4` | `pip install httpx selectolax` | `pip install scrapy` | `pip install playwright` | `pip install pyppeteer` |\n",
    "| 🧠 API Complexity | Simple and intuitive | Requires async programming knowledge | Requires async programming knowledge | Complex (framework, but well-organized) | More powerful, a bit more complex | Simple, Puppeteer-style |\n",
    "| ⏱️ Speed | Moderate for small-scale scraping | 🚀 Very fast (async requests) | 🚀 Very fast (async requests) | 🚀 Very fast (but higher overhead) | ✅ Fast (even with JavaScript pages) | ✅ Moderate (similar to Pyppeteer) |\n",
    "| 📦 Resource Usage | Moderate | Low (async, non-blocking) | Low (async, non-blocking) | High (especially for large projects) | Moderate | Moderate |\n",
    "| 🪟 Handle popups / iframes | ❌ Not applicable | ❌ Not applicable | ❌ Not applicable | ❌ Not applicable | ✅ Excellent support | ✅ Basic support |\n",
    "| 📱 Mobile emulation | ❌ Not applicable | ❌ Not applicable | ❌ Not applicable | ❌ Not applicable | ✅ Easy and built-in | ✅ Possible with extra setup |\n",
    "| 🌍 Geolocation / Permissions | ❌ Not applicable | ❌ Not applicable | ❌ Not applicable | ❌ Not applicable | ✅ Built-in features | ❌ Not directly supported |\n",
    "| 🔍 Code generation tool | ❌ Not available | ❌ Not available | ❌ Not available | ✅ Built-in support for crawling | ✅ `playwright codegen` tool | ❌ Not available |\n",
    "| 🚀 Performance | Good for small to medium-scale tasks | 🚀 Excellent for scraping many pages | 🚀 Excellent for scraping many pages | 🚀 Very good for large projects | ✅ Great for both static and JS-heavy sites | ✅ Good for JS-heavy sites |\n",
    "| 🔒 Anti-bot evasion | ❌ Basic headers for evasion | ❌ Basic headers for evasion | ❌ Basic headers for evasion | ❌ Basic headers for evasion | ✅ Better stealth for JS-heavy sites | ❌ Easily detected (headless Chromium) |\n",
    "| 🛑 Project Status | ✅ Actively maintained | ✅ Actively maintained | ✅ Actively maintained | ✅ Actively maintained | ✅ Actively developed & supported | ❌ No longer actively maintained |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Pros and ❌ Cons Summary\n",
    "\n",
    "#### `requests` + `BeautifulSoup`\n",
    "- ✅ **Very simple** and **easy to use** for beginners\n",
    "- ✅ Well-documented and widely used\n",
    "- ❌ **Not fast** for large-scale scraping (can be slow with 2 million URLs)\n",
    "- ❌ Does **not handle JS** or dynamic content\n",
    "- ❌ Does not support async handling or waiting for many pages to load\n",
    "\n",
    "#### `aiohttp` + `BeautifulSoup`\n",
    "- ✅ **Async** and **very fast** when scraping multiple pages concurrently\n",
    "- ✅ **Low resource usage** (non-blocking)\n",
    "- ❌ Requires **async** programming knowledge\n",
    "- ❌ Does **not handle JS** or dynamic content\n",
    "- ❌ Does not support advanced features like popups, mobile emulation, or permissions\n",
    "\n",
    "\n",
    "#### `Scrapy`\n",
    "- ✅ Best for **large-scale** scraping projects (200,000+ URLs)\n",
    "- ✅ **Built-in crawling** and **link-following** features\n",
    "- ✅ Supports **pipelines** for data cleaning and processing\n",
    "- ✅ **Highly efficient** for handling huge numbers of URLs concurrently (async)\n",
    "- ❌ Overkill for simple scraping tasks\n",
    "- ❌ Does **not handle JS** or dynamic content\n",
    "\n",
    "#### `Playwright (Python)`\n",
    "- ✅ Handles **JavaScript-heavy** pages well and supports **async** functions\n",
    "- ✅ **Fast** and reliable even with dynamic content\n",
    "- ✅ **Multiple browser support** (Chromium, Firefox, Safari)\n",
    "- ✅ **Built-in auto-waiting** for elements, reducing issues with timing\n",
    "- ❌ **Slightly more complex** compared to requests-based solutions\n",
    "- ❌ **Slower** than aiohttp or httpx for static pages but still fast enough for mixed sites\n",
    "\n",
    "#### `Pyppeteer`\n",
    "- ✅ Works well with **JavaScript-heavy sites** using **Chromium** browser\n",
    "- ✅ **Simple** API, **similar to Puppeteer**\n",
    "- ✅ Good for **JavaScript rendering** when simple browser automation is needed\n",
    "- ❌ **Slower** than Playwright and **limited to Chromium**\n",
    "- ❌ Not actively maintained anymore (may have issues with recent updates)\n",
    "- ❌ **Basic anti-bot evasion**, less stealthy compared to Playwright\n",
    "- ❌ **No native support** for multiple browsers like Playwright\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 **Best Methods for Scraping 2,000,000 Mixed HTTPS + JavaScript URLs:**\n",
    "\n",
    "#### **1. For Static HTTPS Pages:**\n",
    "If the pages are **static** (not requiring JS to render), the best approach would be:\n",
    "- **`aiohttp` + `BeautifulSoup`** or **`httpx` + `selectolax`** for **speed** and **low resource usage**. These tools handle many pages concurrently using **async**.\n",
    "\n",
    "#### **2. For JavaScript-Rendered Pages:**\n",
    "- **`Playwright (Python)`** is the **best option** to handle **both static and dynamic pages** for **2,000,000 URLs**. It combines **async support**, **browser automation**, and **dynamic content handling** in one package.\n",
    "- **`Pyppeteer`** is a **good option** if you prefer a **simple API** and are working with **Chromium-only** sites. However, **Playwright** is the better choice if you want **multiple browser support** and more **advanced features** (like handling popups or mobile emulation).\n",
    "\n",
    "#### **3. For Large-Scale Crawling:**\n",
    "- If your project is primarily **large-scale crawling**, **`Scrapy`** is great for **handling 2 million URLs** concurrently and efficiently, with built-in features like **crawling** and **link-following**. However, **`Scrapy`** doesn't handle **JavaScript-rendered pages**, so you'll need to use **Playwright** alongside it for dynamic content.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚡ **Final Thought:**\n",
    "- **Playwright** is the **best choice** for handling **both static and dynamic pages** at scale, especially when scraping a mix of **2,000,000 URLs**. It can easily handle **JavaScript-heavy sites** and has advanced features that make it robust for large projects.\n",
    "- **Pyppeteer** is a **simpler alternative** for JavaScript rendering, but it's **limited to Chromium** and is no longer actively maintained. If you need **more control and performance**, **Playwright** is the better tool.\n",
    "- For **pure static page scraping**, **`aiohttp`** or **`httpx`** will be the fastest and most efficient tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7e23e-c091-4aa7-b156-021762e0fab0",
   "metadata": {},
   "source": [
    "# size_based and Structured_based chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d284dff-3a53-407c-864e-27e7f3c823f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "import hashlib\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "from typing import Dict, List, Optional, Tuple, Set\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema import Document\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import aiohttp\n",
    "import ssl\n",
    "import time\n",
    "import nest_asyncio\n",
    "from langdetect import detect\n",
    "from playwright.async_api import async_playwright\n",
    "import psutil\n",
    "\n",
    "# Remove all existing handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "    \n",
    "logging.basicConfig(\n",
    "    filename='desy_scraper.log',       # ✅ Your log file name\n",
    "    filemode='w',                      # 'w' to overwrite each run, 'a' to append\n",
    "    level=logging.DEBUG,               # Set to DEBUG to capture everything\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "#logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set directories\n",
    "user_data_dir = \"/afs/desy.de/user/t/taheri/scratch/cache/pyppeteer_cache\"\n",
    "download_dir = \"/afs/desy.de/user/t/taheri/scratch/cache/pyppeteer_downloads\"\n",
    "os.makedirs(user_data_dir, exist_ok=True)\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Apply nest_asyncio for Jupyter compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "        # Compile individual patterns for unwanted tags\n",
    "\n",
    "class DESYContentProcessor:\n",
    "    # Constants\n",
    "    MIN_CHUNK_CHARS = 30      # Main threshold for final chunks\n",
    "    MIN_INITIAL_CHARS = 20    # Initial content validation\n",
    "    MIN_TEXT_SAMPLE_LENGTH = 50  # Reduced from 200\n",
    "    MAX_CONTENT_AREA_SIZE = 50000\n",
    "    DEFAULT_TIMEOUT = 300\n",
    "    JS_DETECTION_MIN_LINKS = 3\n",
    "    NON_HTML_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.pdf', \n",
    "                          '.mp4', '.mp3', '.avi', '.mov', '.wmv', '.zip', '.tar', \n",
    "                          '.gz', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', \".xml\"}\n",
    "\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: int,\n",
    "        content_tags: List[str] = [\n",
    "            \"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"ul\", \"ol\",\n",
    "            \"td\", \"th\", \"tr\", \"table\", \"caption\", \"dt\", \"dd\", \"span\",\n",
    "            \"article\", \"section\", \"main\", \"div\",\n",
    "            \"div.teaser-text\", \"div.content\", \"div.text-block\",\n",
    "            \"div.publication-item\", \"div.news-item\", \"div.portlet-body\",\n",
    "            \"div.event-details\", \"div.indico-content\", \"div.publication-list\",\n",
    "            \"div.event-description\", \"div.news-content\", \"div.status-report\",\n",
    "            \"div.status\", \"div.monitor\", \"div.experiment\", \"div.results\",\n",
    "            \"p[id]\", \"table.i-table\", \"div.timetable\"\n",
    "        ],\n",
    "        excluded_keywords: List[str] = [\"cookie\", \"privacy\", \"copyright\", \"disclaimer\", \"login\", \"password\"],\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        batch_size=10,\n",
    "        timeout: int = 300,  # Increased from 30\n",
    "        js_wait_time: int = 10000,  # Increased from 4000\n",
    "        js_scroll: bool = True\n",
    "    ):\n",
    "        self.browser = None\n",
    "        self.context = None\n",
    "        #self.max_workers: int = 5 #48\n",
    "        #self.max_workers = max_workers\n",
    "\n",
    "        logical_cpus = psutil.cpu_count(logical=True)\n",
    "        total_ram_gb = psutil.virtual_memory().total / 1e9\n",
    "        \n",
    "        # 🛠 Tune max_workers based on CPU and RAM\n",
    "        self.max_workers = min(\n",
    "            logical_cpus * 2,                          # Concurrency factor (2x CPUs)\n",
    "            int((total_ram_gb // 2) * logical_cpus),   # Memory-aware cap\n",
    "            200                                        # Optional hard ceiling to avoid overload\n",
    "        )\n",
    "        \n",
    "        # 🎭 Adjust JS rendering separately (lighter by default)\n",
    "        js_slots = max(4, self.max_workers // 6)  # JS rendering is heavier per task\n",
    "       \n",
    "        self.js_semaphore = asyncio.Semaphore(js_slots)\n",
    "#        self.js_semaphore = asyncio.Semaphore(2) # ⚖️ Limits concurrent JS rendering tasks (e.g., n = 5 or 10)\n",
    "\n",
    "\n",
    "\n",
    "        self.browser_lock = asyncio.Lock()       # 🔐 Protects Playwright init and page creation\n",
    "        self.session_lock = asyncio.Lock()       # 🔐 Protects aiohttp session creation\n",
    "        self.url_lock = asyncio.Lock()           # 🔐 Protects shared data: processed_urls, url_to_documents_map, error_urls, redirected_urls\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.cookie_text_patterns = [\n",
    "            r'cookie[- ]?banner',\n",
    "            r'cookie[- ]?consent',\n",
    "            r'diese website verwendet cookies',\n",
    "            r'we use cookies',\n",
    "            r'accept all cookies',\n",
    "            r'cookie einstellungen',\n",
    "            r'cookie policy',\n",
    "            r'consent to cookies',\n",
    "            r'diese seite nutzt cookies',\n",
    "            r'cookie notice',\n",
    "            r'cookie preferences',\n",
    "            r'cookie declaration',\n",
    "            r'cookie information',\n",
    "            r'cookie settings',\n",
    "            r'cookie usage'\n",
    "        ]\n",
    "        \n",
    "        # CRITICAL: These should be applied first as they have the biggest impact\n",
    "        self.critical_patterns = [\n",
    "            # Scripts and styles - highest priority\n",
    "            re.compile(r'<(script|style)[^>]*>.*?</\\1>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Navigation structures - very high impact\n",
    "            re.compile(r'<nav\\b[^>]*>.*?</nav>', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<(?:header|footer)\\b[^>]*>.*?</(?:header|footer)>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Forms - high impact, often contain unwanted elements\n",
    "            re.compile(r'<form\\b[^>]*>.*?</form>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Large container elements by ID - high impact\n",
    "            re.compile(r'<(?:div|section|nav|ul|header)\\b[^>]*id\\s*=\\s*[\\'\"](?:footer|overall|wrapper|icons|search_icon|phone_icon|close_gcs|mobile_menu_header|mobile_menu|mobile_dropdown|mobile_loading|mobile_dropdown_content|top|logoarea|topleft|topright|topmenu|menu|main_menu|header|leftmenu|rightmenu)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</(?:div|section|nav|ul|header)>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Cookie removal patterns - ADDED FROM CODE 1\n",
    "            re.compile(r'<(div|section|aside|footer)[^>]*id=[\"\\']?[^\"\\'>]*\\b(cookie|consent|privacy|banner|notice|preferences)\\b[^\"\\'>]*[\"\\']?[^>]*>.*?</\\1>', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<(div|section|aside|footer)[^>]*class=[\"\\'][^\"\\'>]*\\b(cookie|consent|banner|popup|notice|preferences|privacy|cookie-consent-wrapper|cookie-bar-wrapper)[^\"\\'>]*[\"\\'][^>]*>.*?</\\1>', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<(div|section|aside|footer)[^>]*style=[\"\\'][^\"\\']*display\\s*:\\s*none[^\"\\']*[\"\\'][^>]*>.*?</\\1>', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<[^>]+class=[\"\\'][^\"\\'>]*\\bcookie-bar__inner\\b[^\"\\'>]*[\"\\'][^>]*>.*?</[^>]+>', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<!--\\s*Cookie\\s+Bar\\s*-->.*?<!--\\s*End\\s+Cookie\\s+Bar\\s*-->', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<div[^>]*id=[\"\\']?cookie-bar[\"\\']?[^>]*>.*?</div>', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<nav\\b[^>]*id\\s*=\\s*[\\'\"](?:leftmenu|topmenu|menu)[^\\'\\\"]*[\\'\"][^>]*>.*?</nav>', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<ul\\b[^>]*id\\s*=\\s*[\\'\"](?:main_menu|menu)[^\\'\\\"]*[\\'\"][^>]*>.*?</ul>', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<li\\b[^>]*class\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:inactive|active|ZMSFolder\\d*|ZMSDocument\\d*)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</li>', re.DOTALL | re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        # HIGH: Important patterns for navigation and UI elements\n",
    "        self.high_priority_patterns = [\n",
    "            # Navigation by class - high frequency\n",
    "            re.compile(r'<(?:div|ul|ol|section)\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:breadcrumb|bread[-_]?nav|nav|navigation|tagline|menu[-_]?bar|top[-_]?nav|site[-_]?nav|main[-_]?navigation|nav[-_]?container|sub[-_]?nav|menu[-_]?container|menu|sub[-_]?menu|nav[-_]?menu|quick[-_]?nav|quick[-_]?links)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</(?:div|ul|ol|section)>', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<(?:div|ul|ol|section|li)\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:breadcrumb|bread[-_]?nav|nav|navigation|tagline|menu[-_]?bar|top[-_]?nav|site[-_]?nav|main[-_]?navigation|nav[-_]?container|sub[-_]?nav|menu[-_]?container|menu|sub[-_]?menu|nav[-_]?menu|quick[-_]?nav|quick[-_]?links|topright[-_]?button|wrapper)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</(?:div|ul|ol|section|li)>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Header and footer containers\n",
    "            re.compile(r'<(?:header|footer)\\b[^>]*(?:id\\s*=\\s*[\\'\"]header[\\'\"])?[^>]*>.*?</(?:header|footer)>', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<div\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:header|footer|site[-_]?footer|page[-_]?footer|site[-_]?header|nav[-_]?footer|group[-_]?header|banner[-_]?header|wrapper)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</div>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Cookie and consent banners - high user impact\n",
    "            re.compile(r'<(?:div|section|aside)\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:cookies?|consent|banner|popup|modal|cookie[-_]?notices?|cookie[-_]?consents?|cookie[-_]?policys?|gdpr|privacy[-_]?banner)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</(?:div|section|aside)>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Sidebar and widget areas\n",
    "            re.compile(r'<(?:div|aside|section)\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:sidebar|left|right|side[-_]?nav|widget[-_]?area|nav[-_]?panel)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</(?:div|aside|section)>', re.DOTALL | re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        # MEDIUM: Specific element patterns\n",
    "        self.medium_priority_patterns = [\n",
    "            # Search elements\n",
    "            re.compile(r'<div\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:search|search[-_]?form|search[-_]?box|search[-_]?bar|cse[-_]?search[-_]?form)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</div>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Mobile elements\n",
    "            re.compile(r'<(?:div|nav|ul)\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\bmobile(?:[-_]?(?:nav|menu|back|toggle|dropdown|loading))?\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</(?:div|nav|ul)>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Language switchers\n",
    "            re.compile(r'<(?:div|ul|select)\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:lang|language|lang[-_]?switch)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</(?:div|ul|select)>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Overlays and modals\n",
    "            re.compile(r'<(?:div|section)\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:overlay|modal[-_]?overlay|popup[-_]?overlay)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</(?:div|section)>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Buttons and UI elements\n",
    "            re.compile(r'<(?:button|input|div)\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:btns?|buttons?|btt|topright[-_]?button)\\b[^\\'\\\"]*[\\'\"][^>]*(?:>.*?</(?:button|input|div)>|/??>)', re.DOTALL | re.IGNORECASE),\n",
    "          \n",
    "            # Remove DOI links\n",
    "            re.compile(r'<a\\b[^>]*href\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:doi\\.org|journals\\.aps\\.org|dx\\.doi\\.org|DOI:)[^\\'\\\"]*[\\'\"][^>]*>.*?</a>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Wrapper and container elements that contain navigation - FROM CODE 1\n",
    "            re.compile(r'<(?:div|section)\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:wrapper|container|main[-_]?container|page[-_]?wrapper|site[-_]?wrapper)\\b[^\\'\\\"]*[\\'\"][^>]*>(?:(?!<(?:main|article|content)\\b).)*?</(?:div|section)>', re.DOTALL | re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        # LOW: Fine-grained cleanup patterns\n",
    "        self.low_priority_patterns = [\n",
    "            # Specific list items - lower impact\n",
    "            re.compile(r'<li\\b[^>]*(?:class\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:inactive|folder|nav[-_]?item|menu[-_]?item|ZMSFolder\\d*|ZMSDocument\\d*)\\b[^\\'\\\"]*[\\'\"])?[^>]*>.*?</li>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Footnotes and references\n",
    "            re.compile(r'<(?:div|section|aside|span)\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:footnotes?|foot[-_]?notes?|references?|citations?|endnotes?)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</(?:div|section|aside|span)>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Specific links\n",
    "            re.compile(r'<a\\b[^>]*(?:id\\s*=\\s*[\\'\"](?:mobile_back_to_desy|mobile[-_]?nav[-_]?toggle|search|phone)[\\'\"]|(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:inactive|ZMSFolder\\d*|ZMSDocument\\d*)\\b[^\\'\\\"]*[\\'\"]|href\\s*=\\s*[\\'\"][^\\'\\\"]*(?:index_print|desy\\.de|testbeam\\.desy\\.de)[^\\'\\\"]*[\\'\"]|title\\s*=\\s*[\\'\"][^\\'\\\"]*(?:Change\\s+language|DESY\\s+Homepage|to\\s+[\\'\"]Accelerators[\\'\"])[^\\'\\\"]*[\\'\"]|target\\s*=\\s*[\\'\"]_blank[\\'\"])[^>]*>.*?</a>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Specific images\n",
    "            re.compile(r'<img\\b[^>]*(?:id\\s*=\\s*[\\'\"][^\\'\\\"]*(?:phonebook_icon|print_icon|lang_icon|desylogo)[^\\'\\\"]*[\\'\"]|alt\\s*=\\s*[\\'\"][^\\'\\\"]*(?:phone\\s+book|Diese\\s+Seite\\s+drucken|loading|DESY\\s+Logo)[^\\'\\\"]*[\\'\"]|src\\s*=\\s*[\\'\"][^\\'\\\"]*(?:loading\\.gif|logo_desy\\.gif|arrow_large_white\\.png)[^\\'\\\"]*[\\'\"])[^>]*/?>', re.IGNORECASE),\n",
    "            \n",
    "            # ARIA and accessibility\n",
    "            re.compile(r'<[^>]*(?:role\\s*=\\s*[\\'\"]navigation[\\'\"]|aria-label\\s*=\\s*[\\'\"][^\\'\\\"]*[\\'\"])[^>]*>.*?</[^>]+>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Nested inactive lists - FROM CODE 1\n",
    "            re.compile(r'<ul\\b[^>]*>(?:\\s*<li\\b[^>]*(?:class|id)\\s*=\\s*[\\'\"][^\\'\\\"]*\\b(?:inactive|ZMSFolder\\d*|ZMSDocument\\d*)\\b[^\\'\\\"]*[\\'\"][^>]*>.*?</li>\\s*)+</ul>', re.DOTALL | re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        # SPECIALIZED: Domain-specific patterns\n",
    "        self.specialized_patterns = [\n",
    "            # DESY institutional content - FROM CODE 1\n",
    "            re.compile(r'Deutsches\\s+Elektronen-Synchrotron\\s+DESY\\s+A\\s+Research\\s+Centre\\s+of\\s+the\\s+Helmholtz\\s+Association', re.IGNORECASE),\n",
    "            re.compile(r'Data\\s+Privacy\\s+Policy\\s*\\|\\s*Declaration\\s+of\\s+Accessibility\\s*\\|\\s*Imprint\\s*©[^.]*', re.IGNORECASE),\n",
    "            re.compile(r'A\\s+Research\\s+Centre\\s+of\\s+the\\s+Helmholtz\\s+Association', re.IGNORECASE),\n",
    "            re.compile(r'©\\s*\\d{4}\\s*Deutsches\\s+Elektronen-Synchrotron\\s+DESY.*?(?:Helmholtz\\s+Association)?', re.IGNORECASE),\n",
    "            re.compile(r'Deutsches\\s*Elektronen-Synchrotron', re.IGNORECASE),\n",
    "            re.compile(r'Data\\s+Privacy\\s+Policy\\s*\\|.*?(?:Imprint|©)', re.IGNORECASE),\n",
    "            re.compile(r'Impressum\\s*/\\s*Datenschutz\\s*/\\s*Erklärung\\s+zur\\s+Barrierefreiheit', re.IGNORECASE),\n",
    "            re.compile(r'\\bSprungnavigation\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bZielgruppennavigation\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bServicefunktionen\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bBreadcrumb\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bFooter\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bDesy\\s+Global\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bZum\\s+Untermenü\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bZum\\s+Inhalt\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bZum\\s+Hauptmenu\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bInfos\\s*&\\s*Services\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bLeichte\\s+Sprache\\b', re.IGNORECASE),\n",
    "            re.compile(r'\\bGebärdensprache\\b', re.IGNORECASE)\n",
    "        ]\n",
    "        \n",
    "        # CLEANUP: Final cleanup patterns\n",
    "        self.cleanup_patterns = [\n",
    "            # HTML comments\n",
    "            re.compile(r'<!--\\s*(?://wrapper\\s*//\\s*-->.*?<!--\\s*/standard_html_header\\s*--|/?\\s*standard_html_header\\s*-->)', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<!--[^>]*(?:wrapper|overall|standard_html)[^>]*-->', re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r'<!--[^>]*tal:attributes[^>]*-->', re.IGNORECASE),\n",
    "            re.compile(r'<!--a\\s+tal:.*?</a-->', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # SVG and other media\n",
    "            re.compile(r'<svg[^>]*>.*?</svg>', re.DOTALL | re.IGNORECASE),\n",
    "            \n",
    "            # Attributes and styling\n",
    "            re.compile(r'title\\s*=\\s*[\\'\"][^\\'\\\"]*(?:Aktuelle|Seminare|Events)[^\\'\\\"]*[\\'\"]', re.IGNORECASE),\n",
    "            re.compile(r'<[^>]*style\\s*=\\s*[\\'\"][^\\'\\\"]*(?:display\\s*:\\s*block|text-align\\s*:\\s*right|margin|opacity)[^\\'\\\"]*[\\'\"][^>]*>', re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        # TEXT-BASED: Patterns for text content cleanup - ENHANCED FROM CODE 1\n",
    "        self.text_cleanup_patterns = [\n",
    "            \n",
    "        #safe_patterns \n",
    "        re.compile(r'\\bNavigation\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\bDatenschutzerklärung\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\bErklärung\\s+zur\\s+Barrierefreiheit\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\bBack\\s+to\\s+Top\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(?:nav|menu|breadcrumb|navigation)\\s*[:\\-\\|]\\s*', re.IGNORECASE),\n",
    "\n",
    "        #moderate_patterns \n",
    "        re.compile(r'\\b(?:Home|Startseite|Kontakt|Suche|Login|Anmelden)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(?:Archiv|Archive)\\s*\\d{4}', re.IGNORECASE),\n",
    "        re.compile(r'\\b(?:Page\\s+\\d+|Seite\\s+\\d+|\\d+\\s+of\\s+\\d+)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(?:cookie|gdpr|popup|consent)\\b', re.IGNORECASE),\n",
    "    \n",
    "        #aggressive_patterns\n",
    "        # re.compile(r'\\b(?:LinkedIn|Twitter|Facebook|Instagram)\\b', re.IGNORECASE),\n",
    "        # re.compile(r'\\b(?:News|Events?|Seminare|Aktuelles)\\b', re.IGNORECASE),\n",
    "        # re.compile(r'\\b(?:About|Über\\s+uns|Presse\\s*&\\s*Kommunikation)\\b', re.IGNORECASE),\n",
    "        # re.compile(r'\\b(?:DOOR\\s+-\\s+DESY|User\\s+consortia|Sample\\s+Environment)\\b', re.IGNORECASE),\n",
    "        # re.compile(r'\\b(?:index_ger|index_en|[A-Za-z]*Folder\\d*)\\b', re.IGNORECASE),\n",
    "        ]\n",
    "\n",
    " \n",
    "        \n",
    "        # Whitespace pattern - FROM CODE 1 (SEPARATE FOR EFFICIENCY)\n",
    "        self.whitespace_pattern = re.compile(r'[\\xa0\\u202f\\n\\r\\t\\s]+')\n",
    "        \n",
    "       \n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.content_tags = content_tags\n",
    "        self.excluded_keywords = excluded_keywords\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.batch_size = batch_size\n",
    "        self.timeout = timeout\n",
    "        self.processed_hashes: Set[str] = set()\n",
    "        self.full_text_hashes: Set[str] = set()\n",
    "\n",
    "        self.processed_urls: Set[str] = set()\n",
    "        self.max_hashes = 100000  # Limit to 100,000 hashes\n",
    "        self.max_urls = 10000     # Limit to 10,000 URLs\n",
    "    \n",
    "        self.error_urls: Dict[str, str] = {}\n",
    "        # ADD THIS LINE: Track redirected URLs separately for better debugging\n",
    "        self.redirected_urls: Dict[str, str] = {}\n",
    "        self.js_wait_time = js_wait_time\n",
    "        self.js_scroll = js_scroll\n",
    "        \n",
    "        self.session = None\n",
    "        self.progress_bar = None\n",
    "        self.ssl_bypass_domains = set()\n",
    "        self.url_to_documents_map = {}    \n",
    "\n",
    "        self.page_character_counts = {}  # Add this line to track character counts per page\n",
    "  \n",
    "\n",
    "\n",
    "        self.domain_configs = {\n",
    "            \"petra3.desy.de\": {\"timeout\": 500, \"max_connections\": 2, \"retry_delay\": 3, \"js_wait_time\": 12000},\n",
    "            \"indico.desy.de\": {\"timeout\": 500, \"max_connections\": 2, \"retry_delay\": 5, \"js_wait_time\": 15000},\n",
    "            \"pitz.desy.de\": {\"timeout\": 500, \"max_connections\": 2, \"retry_delay\": 3, \"js_wait_time\": 12000}        }\n",
    "        \n",
    "\n",
    "        self.domain_configs.update({\n",
    "            \"www.desy.de\": { \"timeout\": 900, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 60000},\n",
    "            \"desy.de\": {\"timeout\": 500, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 30000},\n",
    "            \"newsletter.desy.de\": { \"timeout\": 900, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 60000},\n",
    "            \"connect.desy.de\": { \"timeout\": 500, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 30000},\n",
    "            \"astroparticle-physics.desy.de\": { \"timeout\": 500, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 30000},\n",
    "            \"innovation.desy.de\": { \"timeout\": 500, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 30000},\n",
    "            \"petra4.desy.de\": { \"timeout\": 900, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 90000},\n",
    "            \"accelerators.desy.de\": { \"timeout\": 900, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 60000},\n",
    "            \"v22.desy.de\": { \"timeout\": 500, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 30000},\n",
    "            \"photon-science.desy.de\": { \"timeout\": 500, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 30000},\n",
    "            \"particle-physics.desy.de\": { \"timeout\": 900, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 60000},\n",
    "            \"pr.desy.de\": { \"timeout\": 500, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 30000},\n",
    "            \"fh.desy.de\": { \"timeout\": 900, \"max_connections\": 1, \"retry_delay\": 3, \"js_wait_time\": 60000}\n",
    "        })\n",
    "        #\"requires_js\": False ,\n",
    "        self.default_domain_config = { \"timeout\": timeout, \"max_connections\": 10, \"retry_delay\": 2}  # Reduced from 20\n",
    "        \n",
    "#################################check4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.debug_mode = False #True #False\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def add_to_processed_hashes(self, content_hash: str):\n",
    "        if len(self.processed_hashes) >= self.max_hashes:\n",
    "            logger.warning(\"Processed hashes limit reached, clearing oldest entries\")\n",
    "            self.processed_hashes = set(list(self.processed_hashes)[self.max_hashes//2:])\n",
    "        self.processed_hashes.add(content_hash)\n",
    "    \n",
    "    def add_to_processed_urls(self, url: str):\n",
    "        if len(self.processed_urls) >= self.max_urls:\n",
    "            logger.warning(\"Processed URLs limit reached, clearing oldest entries\")\n",
    "            self.processed_urls = set(list(self.processed_urls)[self.max_urls//2:])\n",
    "        self.processed_urls.add(url)\n",
    "\n",
    "    \n",
    "   \n",
    "    def track_page_character_count(self, url: str, content: str, title: str = \"\", language: str = \"en\", depth: int = 0):\n",
    "        if len(content) < self.MIN_CHUNK_CHARS:\n",
    "            logger.warning(f\"Skipping character count tracking for {url}: Content length {len(content)} below threshold\")\n",
    "            return\n",
    "        self.page_character_counts[url] = {\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'character_count': len(content),\n",
    "            'word_count': len(content.split()) if content else 0,\n",
    "            'language': language,\n",
    "            'depth': depth\n",
    "           # 'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    \n",
    "    def should_skip_url(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL should be skipped based on file extension.\"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path.lower()\n",
    "        \n",
    "        # Check if URL ends with any non-HTML extension\n",
    "        for ext in self.NON_HTML_EXTENSIONS:\n",
    "            if path.endswith(ext):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    def detect_language(self, soup: BeautifulSoup, text_sample: str, url: str = None) -> str:\n",
    "        # Log URL for context\n",
    "        \n",
    "        \n",
    "        # Check if URL ends with _ger.html\n",
    "        if url and url.lower().endswith('_ger.html'):\n",
    "            #logger.info(f\"URL {url} identified as German due to _ger.html suffix\")\n",
    "            return 'de'\n",
    "                \n",
    "        # Prioritize langdetect for sufficient text\n",
    "        if text_sample and len(text_sample) >= 50:\n",
    "            try:\n",
    "                detected_lang = detect(text_sample[:1000])\n",
    "                #logger.info(f\"Language detected via langdetect for {url}: {detected_lang}\")\n",
    "                return detected_lang\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Langdetect failed for {url}: {e}\")\n",
    "        \n",
    "        # Fallback to HTML attributes\n",
    "        html_lang = None\n",
    "        if soup.html and soup.html.get('lang'):\n",
    "            html_lang = soup.html.get('lang').strip().lower()\n",
    "            logger.debug(f\"HTML lang attribute: {html_lang}\")\n",
    "        if not html_lang and soup.html and soup.html.get('xml:lang'):\n",
    "            html_lang = soup.html.get('xml:lang').strip().lower()\n",
    "            logger.debug(f\"XML lang attribute: {html_lang}\")\n",
    "        if not html_lang:\n",
    "            meta_lang = soup.find('meta', attrs={'http-equiv': 'content-language'})\n",
    "            if meta_lang and meta_lang.get('content'):\n",
    "                html_lang = meta_lang.get('content').strip().lower()\n",
    "                logger.debug(f\"Meta content-language: {html_lang}\")\n",
    "        if not html_lang:\n",
    "            meta_lang = soup.find('meta', attrs={'property': 'og:locale'})\n",
    "            if meta_lang and meta_lang.get('content'):\n",
    "                html_lang = meta_lang.get('content').strip().lower()\n",
    "                logger.debug(f\"Meta og:locale: {html_lang}\")\n",
    "        if html_lang:\n",
    "            html_lang = re.sub(r'[^a-z]', '', html_lang.split('-')[0].lower())\n",
    "            if len(html_lang) == 2:\n",
    "                #logger.info(f\"Language from HTML attributes for {url}: {html_lang}\")\n",
    "                return html_lang\n",
    "        \n",
    "        # Final fallback\n",
    "        logger.warning(f\"No language detected for {url}, defaulting to 'en'\")\n",
    "        return 'en'\n",
    "\n",
    "    def _apply_pattern_group(self, text: str, patterns: list, group_name: str) -> str:\n",
    "        \"\"\"Apply a group of patterns efficiently with optional debugging.\"\"\"\n",
    "        if not patterns:\n",
    "            return text\n",
    "            \n",
    "        total_matches = 0\n",
    "        for i, pattern in enumerate(patterns):\n",
    "            if self.debug_mode:\n",
    "                matches = len(pattern.findall(text))\n",
    "                total_matches += matches\n",
    "                if matches > 0:\n",
    "                    logger.debug(f\"{group_name}[{i}]: {matches} matches\")\n",
    "            \n",
    "            text = pattern.sub('', text)\n",
    "        \n",
    "        # if self.debug_mode and total_matches > 0:\n",
    "        #     logger.info(f\"{group_name}: {total_matches} total matches\")\n",
    "        \n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def clean_content(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "                  \n",
    "        try:\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            \n",
    "            main_content_selectors = [\n",
    "                # Semantic HTML5 tags (most reliable)\n",
    "                'main', 'article', 'section[class*=\"content\"]',\n",
    "            \n",
    "                #  Highly specific content blocks\n",
    "                'div[class*=\"main-content\"]',\n",
    "                'div[class*=\"content-section\"]',\n",
    "                'div[class*=\"text-block\"]',\n",
    "            \n",
    "                #  Common structured content containers\n",
    "                'div[id=\"content\"]', 'div[id=\"main\"]', 'div[id=\"bodyContent\"]',\n",
    "                'div[class*=\"content\"]', 'div[class*=\"text\"]', 'div[class*=\"body\"]',\n",
    "            \n",
    "                #  Layout wrappers (lower priority)\n",
    "                'div[class*=\"page\"]',\n",
    "                'div[class*=\"container\"]',\n",
    "            \n",
    "                #  Non-semantic but sometimes useful\n",
    "                'center'\n",
    "            ]\n",
    "\n",
    "\n",
    "            main_content = None\n",
    "            for selector in main_content_selectors:\n",
    "                main_content = soup.select_one(selector)\n",
    "               \n",
    "    \n",
    "            if not main_content:\n",
    "                main_content = soup.body or soup\n",
    "             \n",
    "    \n",
    "            #  Move structural cleanup here — before converting to string\n",
    "            selectors = [\n",
    "                'div[id=\"overall\"]', 'div[class=\"wrapper\"]', 'header[id=\"header\"]',\n",
    "                'div[id=\"mobile_menu_header\"]', 'div[id=\"mobile_menu\"]', 'div[id=\"mobile_dropdown\"]',\n",
    "                'div[id=\"top\"]', 'div[id=\"logoarea\"]', 'div[id=\"topleft\"]', 'div[id=\"topright\"]',\n",
    "                'div[id=\"topmenu\"]', 'nav[id=\"menu\"]', 'ul[id=\"main_menu\"]',\n",
    "                'nav', 'ul[id*=\"menu\" i]', 'ol[id*=\"menu\" i]',\n",
    "                'div[id=\"icons\"]', 'div[class=\"topright_button\"]',\n",
    "                'li[class*=\"ZMS\"]', 'a[class*=\"ZMS\"]',\n",
    "                'img[class=\"imgNoborder\"]', 'img[id*=\"logo\"]', 'img[id*=\"icon\"]',\n",
    "                'a[target=\"_blank\"]', 'a[href*=\"doi.org\"]', 'a[href*=\"DOI\"]',\n",
    "                'a[href*=\"journals.aps.org\"]', 'a[href*=\"dx.doi.org\"]', 'a[href*=\"doi:\"]',\n",
    "                'a[href*=\"abstract\"]', 'a[href*=\"citation\"]',\n",
    "                'div[class=\"clear\"]', 'div[class=\"loading\"]',\n",
    "                'footer', 'div[id*=\"footer\" i]', 'div[class*=\"footer\" i]', 'div[class*=\"copyright\" i]',\n",
    "                'div[class*=\"teaser\" i]', 'div[class*=\"LinkElement\" i]', 'div[class*=\"quicklinks\" i]', \n",
    "                'div[class*=\"ZMS\" i]', 'div[id*=\"teaser\" i]', 'div[id*=\"quicklinks\" i]',\n",
    "                '[data-cookie]', '[data-consent]', '[class*=\"cookie\" i]', '[class*=\"consent\" i]', \n",
    "                '[style*=\"display:none\" i]', '[style*=\"visibility:hidden\" i]',\n",
    "                'div[id=\"quick_nav_container\"]',\n",
    "                'a[href*=\"data_privacy_policy\"]', 'a[href*=\"declaration_of_accessibility\"]',\n",
    "                'ul[style*=\"padding-bottom\"]',\n",
    "                'button[class*=\"btt\"]', 'div[class*=\"btt\"]',\n",
    "                'ul[class*=\"footer__links\"]', 'div[class*=\"footer__logos\"]',\n",
    "                'img[alt*=\"Logo\"]', 'a[href*=\"linkedin\"]', 'a[href*=\"twitter\"]',\n",
    "                'li[class*=\"ZMSFolder\"]', 'li[class*=\"ZMSDocument\"]',\n",
    "                'a[class*=\"ZMSFolder\"]', 'a[class*=\"ZMSDocument\"]',\n",
    "                'p.hidden.showforprint',\n",
    "                'p[class*=\"hidden\"][class*=\"showforprint\" i]',\n",
    "                '[class*=\"showforprint\" i]', '[class*=\"show-for-print\" i]',\n",
    "                '[class*=\"hidden\" i][class*=\"print\" i]',\n",
    "                '[class~=\"showforprint\"]', '[class~=\"hidden\"]',\n",
    "                'a[class*=\"print\" i]', 'a[class*=\"changelang\" i]',\n",
    "                    #  Enhanced nav/menu cleanup\n",
    "                'nav', 'header', 'footer',\n",
    "                'div[class*=\"nav\" i]', 'div[id*=\"nav\" i]',\n",
    "                'div[class*=\"menu\" i]', 'div[id*=\"menu\" i]',\n",
    "                'ul[class*=\"menu\" i]', 'ul[id*=\"menu\" i]',\n",
    "                'li[class*=\"menu\" i]', 'li[id*=\"menu\" i]',\n",
    "                'a[class*=\"menu\" i]', 'a[id*=\"menu\" i]',\n",
    "                'section[class*=\"nav\" i]', 'section[class*=\"menu\" i]',\n",
    "                'ul[class*=\"nav\" i]',  # catches <ul class=\"nav\">\n",
    "                'ul[id*=\"nav\" i]',     # just in case\n",
    "                'div[id*=\"content-nav\" i]',  # catches <div id=\"content-nav\">\n",
    "                'div[id=\"page-footer\"]',\n",
    "                'ul[id=\"footer-nav\"]',\n",
    "            ]\n",
    "\n",
    "            for selector in selectors:\n",
    "                elements = set(main_content.select(selector)) | set(soup.select(selector))\n",
    "                #elements = set(main_content.select(selector))\n",
    "               \n",
    "                # if elements:\n",
    "                #     print(f\"Selector '{selector}' matched {len(elements)} elements\")\n",
    "                for element in elements:\n",
    "                    if element.extractable:\n",
    "                        element.decompose()\n",
    "    \n",
    "            # 🧼 Handle lingering <li> outside content manually\n",
    "            for li in main_content.find_all(\"li\"):\n",
    "                if not li.find_parent(id=\"content\"):\n",
    "                    li.decompose()\n",
    "    \n",
    "            # 🔧 Remove DOI links before conversion\n",
    "            doi_href_pattern = re.compile(r'(doi\\.org|journals\\.aps\\.org|dx\\.doi\\.org|DOI:)', re.IGNORECASE)\n",
    "            for a_tag in main_content.find_all('a', href=True):\n",
    "                if doi_href_pattern.search(a_tag['href']):\n",
    "                    if self.debug_mode:\n",
    "                        logger.debug(f\"Removing DOI link: {a_tag}\")\n",
    "                    a_tag.decompose()\n",
    "    \n",
    "            text = str(main_content)\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Main content extraction failed: {e}\")\n",
    "            text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "        # 🔧 Regex pattern cleaning\n",
    "        text = self._apply_pattern_group(text, self.critical_patterns, \"CRITICAL\")\n",
    "        text = self._apply_pattern_group(text, self.high_priority_patterns, \"HIGH\")\n",
    "        text = self._apply_pattern_group(text, self.medium_priority_patterns, \"MEDIUM\")\n",
    "        text = self._apply_pattern_group(text, self.low_priority_patterns, \"LOW\")\n",
    "        text = self._apply_pattern_group(text, self.specialized_patterns, \"SPECIALIZED\")\n",
    "        text = self._apply_pattern_group(text, self.cleanup_patterns, \"CLEANUP\")\n",
    "    \n",
    "        # 🧼 Final DOM pass if any HTML remains\n",
    "        if '<' in text and '>' in text:\n",
    "            try:\n",
    "                soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "                for el in soup.find_all(text=re.compile(r'©\\s*\\d{4}\\s*Deutsches\\s*Elektronen-Synchrotron\\s*DESY', re.I)):\n",
    "                    el.replace_with('')\n",
    "    \n",
    "                for el in soup.find_all(text=True):\n",
    "                    text_content = (el.string or \"\").lower()\n",
    "                    for pattern in self.cookie_text_patterns:\n",
    "                        if re.search(pattern, text_content, re.I):\n",
    "                            parent = el.parent\n",
    "                            for _ in range(4):\n",
    "                                if parent and parent.name in ['div', 'section', 'aside', 'p', 'span']:\n",
    "                                    if self.debug_mode:\n",
    "                                        logger.debug(f\"Removed parent {parent.name} with cookie text\")\n",
    "                                    parent.decompose()\n",
    "                                    break\n",
    "                                parent = parent.parent if parent else None\n",
    "                            break\n",
    "    \n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"HTML parsing failed: {e}\")\n",
    "                text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "        # Final cleanup\n",
    "        text = self._apply_pattern_group(text, self.text_cleanup_patterns, \"TEXT_CLEANUP\")\n",
    "        text = self.whitespace_pattern.sub(' ', text)\n",
    "    \n",
    "        # 🔁 Remove duplicate DOIs\n",
    "        doi_pattern = re.compile(r'\\b10\\.\\d{4,9}/[-._;()/:A-Z0-9]+\\b', re.IGNORECASE)\n",
    "        seen_dois = set()\n",
    "    \n",
    "        def replace_doi(match):\n",
    "            doi = match.group(0)\n",
    "            if doi in seen_dois:\n",
    "                if self.debug_mode:\n",
    "                    logger.debug(f\"Removed duplicate DOI: {doi}\")\n",
    "                return ''\n",
    "            seen_dois.add(doi)\n",
    "            return doi\n",
    "    \n",
    "        text = doi_pattern.sub(replace_doi, text)\n",
    "    \n",
    "        if self.debug_mode:\n",
    "            #logger.setLevel(logging.DEBUG)\n",
    "\n",
    "            problematic_terms = ['<', '>', 'nav', 'menu', 'cookie', 'consent', 'cookie-consent', 'cookie-banner']\n",
    "            found_terms = [term for term in problematic_terms if term in text.lower()]\n",
    "            if found_terms:\n",
    "                logger.warning(f\"Residual unwanted content detected: {', '.join(found_terms)}\")\n",
    "                for term in found_terms:\n",
    "                    if term in ['<', '>']:\n",
    "                        continue\n",
    "                    pattern = re.compile(rf'.{{0,150}}{re.escape(term)}.{{0,150}}', re.IGNORECASE)\n",
    "                    matches = pattern.findall(text)\n",
    "                    if matches:\n",
    "                        logger.debug(f\"Context for '{term}': {matches[:3]}\")\n",
    "                        #logger.warning(f\"Context for '{term}': {matches[:3]}\")\n",
    "                text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def is_login_page(self, soup: BeautifulSoup) -> bool:\n",
    "        \"\"\"Check if the page is a login page.\"\"\"\n",
    "        login_indicators = [\n",
    "            soup.find('form', {'id': lambda x: x and 'login' in x.lower()}),\n",
    "            soup.find('form', {'action': lambda x: x and 'login' in x.lower()}),\n",
    "            soup.find('input', {'name': 'username'}),\n",
    "            soup.find('input', {'name': 'password', 'type': 'password'}),\n",
    "            soup.find('button', text=re.compile(r'log\\s*in|sign\\s*in', re.I)),\n",
    "            soup.find('input', {'value': re.compile(r'log\\s*in|sign\\s*in', re.I)}),\n",
    "            soup.find('div', class_=['login-box', 'auth-form']),  # ← ADDED\n",
    "            soup.find('a', text=re.compile(r'log\\s*in|sign\\s*in|authenticate', re.I))  # ← ADDED\n",
    "        ]\n",
    "       #return soup.title and re.search(r'log\\s*in|sign\\s*in', soup.title.text, re.I) or any(login_indicators)\n",
    "        # Check page title or presence of login indicators\n",
    "        title_matches = soup.title and re.search(r'log\\s*in|sign\\s*in', soup.title.text, re.I)\n",
    "        return title_matches or any(login_indicators)\n",
    "\n",
    "    \n",
    "    def is_not_found_page(self, soup: BeautifulSoup) -> bool:\n",
    "        \"\"\"Check if the page is a 'not found' page.\"\"\"\n",
    "        error_phrases = [\n",
    "            'not found', 'page doesn\\'t exist', '404', 'page not found', \n",
    "            'does not exist', 'could not be found', 'site error', \n",
    "            'error was encountered', 'error occurred'\n",
    "        ]\n",
    "        page_text = soup.get_text(strip=True).lower()\n",
    "    \n",
    "        # Check title for error phrases\n",
    "        if soup.title and any(phrase in soup.title.text.lower() for phrase in error_phrases):\n",
    "            return True\n",
    "\n",
    "\n",
    "        if re.search(r'error.*encountered.*publishing', page_text, re.I):\n",
    "            return True\n",
    "\n",
    "\n",
    "        # Check headings for error phrases\n",
    "        for heading in soup.find_all(['h1', 'h2', 'h3']):\n",
    "            if any(phrase in heading.get_text(strip=True).lower() for phrase in error_phrases):\n",
    "                return True\n",
    "        \n",
    "        # Check entire page text for error phrases\n",
    "        if any(phrase in page_text for phrase in error_phrases):\n",
    "            return True\n",
    "        \n",
    "        # Check if page content is too short (indicates error page)\n",
    "        if len(page_text) < self.MIN_TEXT_SAMPLE_LENGTH:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def extract_list_metadata(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract metadata from structured lists.\"\"\"\n",
    "        content_parts = []\n",
    "        \n",
    "        # Find lists that might contain publication info\n",
    "        \n",
    "        lists = soup.find_all(['ul', 'ol', 'dl'], class_=['publication-list', 'pub-list'])\n",
    "\n",
    "        for list_elem in lists:\n",
    "            items = list_elem.find_all(['li', 'dt', 'dd'])\n",
    "            if len(items) > 1:  # Substantial list-Relaxed from 3\n",
    "                list_content = []\n",
    "                for item in items:\n",
    "                    item_text = item.get_text(strip=True)\n",
    "                    if item_text and len(item_text) > 10:  # Meaningful content\n",
    "                        # Check if item contains metadata indicators\n",
    "                        if any(keyword in item_text.lower() for keyword in \n",
    "                              ['author', 'title', 'journal', 'doi', 'isbn', 'vol', 'pp', 'year', '20']):\n",
    "                            list_content.append(item_text)\n",
    "                \n",
    "                if len(list_content) > 2:  # Multiple metadata items\n",
    "                    content_parts.extend(list_content)\n",
    "        \n",
    "        return \"\\n\".join(content_parts)\n",
    "\n",
    "    def extract_table_metadata(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract publication metadata from tables.\"\"\"\n",
    "        content_parts = []\n",
    "        \n",
    "        # Find tables containing publication data\n",
    "        tables = soup.find_all('table')\n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            if len(rows) < 2:  # Skip empty tables\n",
    "                continue\n",
    "                \n",
    "            # Check if this looks like a publication table\n",
    "            table_text = table.get_text(strip=True).lower()\n",
    "            if any(keyword in table_text for keyword in ['author', 'title', 'journal', 'publication', 'presenter', 'date', 'conference']):\n",
    "                \n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    if len(cells) > 1:  # Multi-column row\n",
    "                        cell_texts = []\n",
    "                        for cell in cells:\n",
    "                            cell_text = cell.get_text(strip=True)\n",
    "                            if cell_text and len(cell_text) > 3:  # Meaningful content\n",
    "                                cell_texts.append(cell_text)\n",
    "                        \n",
    "                        if cell_texts:\n",
    "                            # Format as: \"Title | Author | Journal\" etc.\n",
    "                            content_parts.append(\" | \".join(cell_texts))\n",
    "            else:\n",
    "                # Even if no keywords, extract substantial table content\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    if len(cells) > 1:\n",
    "                        cell_texts = [cell.get_text(strip=True) for cell in cells if cell.get_text(strip=True)]\n",
    "                        if len(cell_texts) > 1 and any(len(text) > 15 for text in cell_texts):\n",
    "                            content_parts.append(\" | \".join(cell_texts))\n",
    "        \n",
    "        return \"\\n\".join(content_parts)\n",
    "    \n",
    "        \n",
    "    def extract_content(self, soup: BeautifulSoup, use_tags: bool = True) -> Tuple[str, str]:\n",
    "        \"\"\"Single-strategy content extraction with proper cleaning and deduplication\"\"\"\n",
    "        if not soup:\n",
    "            logger.warning(\"No soup provided for content extraction\")\n",
    "            return \"\", \"\"\n",
    "    \n",
    "        all_text_parts = []\n",
    "        processed_elements = set()\n",
    "        seen_text_hashes = set()\n",
    "    \n",
    "        # 🔧 Expanded early cleanup: Remove known footer, legal, and social media elements\n",
    "        selectors = (    \n",
    "            '[id*=\"nav\" i], [class*=\"nav\" i], '\n",
    "            '[id*=\"menu\" i], [class*=\"menu\" i], '\n",
    "            '[id*=\"sidebar\" i], [class*=\"sidebar\" i], '\n",
    "            '[id*=\"quicklinks\" i], [class*=\"quicklinks\" i], '\n",
    "            'p.copyright, div.copyright, footer, '\n",
    "            '[class*=\"footer\" i], [id*=\"footer\" i], '\n",
    "            '[class*=\"impressum\" i], [id*=\"impressum\" i], '\n",
    "            '[class*=\"datenschutz\" i], [id*=\"datenschutz\" i], '\n",
    "            '[class*=\"legal\" i], [id*=\"legal\" i], '\n",
    "            '[class*=\"social\" i], [id*=\"social\" i], '\n",
    "            '[class*=\"share\" i], [id*=\"share\" i], '\n",
    "            '[class*=\"links\" i], [id*=\"links\" i], '\n",
    "            '[class*=\"bottom\" i], [id*=\"bottom\" i], '\n",
    "            '[class*=\"contact\" i], [id*=\"contact\" i], '\n",
    "            '[class*=\"mastodon\" i], [class*=\"facebook\" i], '\n",
    "            '[class*=\"instagram\" i], [class*=\"linkedin\" i], '\n",
    "            '[class*=\"twitter\" i], [class*=\"rss\" i], '\n",
    "            'a[href*=\"impressum\"], a[href*=\"datenschutz\"], '\n",
    "            'a[href*=\"privacy\"], a[href*=\"accessibility\"], '\n",
    "            'a[href*=\"kontakt\"], a[href*=\"contact\"], '\n",
    "            'a[href*=\"social\"], a[href*=\"linkedin\"], '\n",
    "            'a[href*=\"twitter\"], a[href*=\"facebook\"], '\n",
    "            'a[href*=\"instagram\"], a[href*=\"mastodon\"], '\n",
    "            'a[href*=\"rss\"]'            \n",
    "        )\n",
    "        for nav_elem in soup.select(selectors):\n",
    "            nav_elem.decompose()\n",
    "    \n",
    "        def should_skip_element(element):\n",
    "            return (\n",
    "                element.get('id') in ['cookie-bar', 'footer', 'page-footer', 'site-footer'] or \n",
    "                any(cls in element.get('class', []) for cls in [\n",
    "                    'cookie-bar', 'LinkElementTitle', 'ZMSTeaserContainer', 'footer', 'copyright',\n",
    "                    'link', 'site-footer', 'ZMSDocument0'\n",
    "                ]) or \n",
    "                element.name == 'li' or \n",
    "                element.find_parent('li') or \n",
    "                element.find_parent(attrs={'id': re.compile(r'(footer|page-footer|site-footer)', re.I)})\n",
    "            )\n",
    "    \n",
    "        comprehensive_tags = [\n",
    "            'p[id]', 'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n",
    "            'div.content-section', 'div.module', 'div.text', 'div.content', \n",
    "            'div.text-block', 'div.main-content', 'div.publication-item',\n",
    "            'div.news-item', 'div.event-details', 'div.news-content',\n",
    "            'div.status-report', 'div.status', 'div.monitor',\n",
    "            *self.content_tags,\n",
    "            'table', 'table.i-table', 'caption', 'td', 'th', 'tr',\n",
    "            'section', 'article', 'main', 'span', 'div'\n",
    "        ]\n",
    "    \n",
    "        for tag in comprehensive_tags:\n",
    "            if \".\" in tag:\n",
    "                tag_name, tag_class = tag.split(\".\", 1)\n",
    "                elements = soup.find_all(tag_name, class_=tag_class)\n",
    "            elif tag.startswith('p['):\n",
    "                elements = soup.find_all('p', id=True)\n",
    "            else:\n",
    "                elements = soup.find_all(tag)\n",
    "    \n",
    "            for element in elements:\n",
    "                if id(element) in processed_elements or should_skip_element(element):\n",
    "                    continue\n",
    "                if any(id(ancestor) in processed_elements for ancestor in element.parents):\n",
    "                    continue\n",
    "                if any(id(descendant) in processed_elements for descendant in element.descendants if hasattr(descendant, 'name')):\n",
    "                    continue\n",
    "    \n",
    "                raw_html = str(element)\n",
    "                cleaned_text = self.clean_content(raw_html)\n",
    "    \n",
    "                cleaned_text = self._apply_pattern_group(cleaned_text, self.critical_patterns, \"CRITICAL\")\n",
    "                cleaned_text = self._apply_pattern_group(cleaned_text, self.high_priority_patterns, \"HIGH\")\n",
    "                cleaned_text = self._apply_pattern_group(cleaned_text, self.medium_priority_patterns, \"MEDIUM\")\n",
    "                cleaned_text = self._apply_pattern_group(cleaned_text, self.low_priority_patterns, \"LOW\")\n",
    "                cleaned_text = self._apply_pattern_group(cleaned_text, self.specialized_patterns, \"SPECIALIZED\")\n",
    "                cleaned_text = self._apply_pattern_group(cleaned_text, self.cleanup_patterns, \"CLEANUP\")\n",
    "    \n",
    "                if not cleaned_text or len(cleaned_text) < self.MIN_CHUNK_CHARS:\n",
    "                    continue\n",
    "    \n",
    "                normalized_for_hash = re.sub(r'\\s+', ' ', cleaned_text.lower().strip())\n",
    "                content_hash = hashlib.md5(normalized_for_hash.encode()).hexdigest()\n",
    "                if content_hash in seen_text_hashes:\n",
    "                    continue\n",
    "    \n",
    "                seen_text_hashes.add(content_hash)\n",
    "                processed_elements.add(id(element))\n",
    "                for descendant in element.descendants:\n",
    "                    if hasattr(descendant, 'name'):\n",
    "                        processed_elements.add(id(descendant))\n",
    "    \n",
    "                all_text_parts.append(cleaned_text)\n",
    "    \n",
    "                if self.debug_mode:\n",
    "                    logger.debug(f\"Added cleaned text from <{element.name}> (tag: {tag}), length: {len(cleaned_text)}\")\n",
    "                    logger.debug(f\"Preview: {cleaned_text[:200]}\")\n",
    "    \n",
    "        content = \"\\n\".join(all_text_parts)\n",
    "    \n",
    "        # 🧼 Final DOM-based cleanup (same as clean_content)\n",
    "        try:\n",
    "            soup_final = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "            for el in soup_final.find_all(text=re.compile(r'©\\s*\\d{4}.*?DESY', re.I)):\n",
    "                el.replace_with('')\n",
    "    \n",
    "            for el in soup_final.find_all(text=True):\n",
    "                text_content = (el.string or \"\").lower()\n",
    "                for pattern in self.cookie_text_patterns:\n",
    "                    if re.search(pattern, text_content, re.I):\n",
    "                        parent = el.parent\n",
    "                        for _ in range(4):\n",
    "                            if parent and parent.name in ['div', 'section', 'aside', 'p', 'span']:\n",
    "                                if self.debug_mode:\n",
    "                                    logger.debug(f\"Removed parent {parent.name} with cookie text\")\n",
    "                                parent.decompose()\n",
    "                                break\n",
    "                            parent = parent.parent if parent else None\n",
    "                        break\n",
    "    \n",
    "            content = soup_final.get_text(separator=' ', strip=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Final HTML parsing failed: {e}\")\n",
    "    \n",
    "        # Final regex cleanup\n",
    "        content = self._apply_pattern_group(content, self.text_cleanup_patterns, \"TEXT_CLEANUP\")\n",
    "        content = self.whitespace_pattern.sub(' ', content)\n",
    "    \n",
    "        # Remove duplicate DOIs\n",
    "        doi_pattern = re.compile(r'\\b10\\.\\d{4,9}/[-._;()/:A-Z0-9]+\\b', re.IGNORECASE)\n",
    "        seen_dois = set()\n",
    "    \n",
    "        def replace_doi(match):\n",
    "            doi = match.group(0)\n",
    "            if doi in seen_dois:\n",
    "                if self.debug_mode:\n",
    "                    logger.debug(f\"Removed duplicate DOI: {doi}\")\n",
    "                return ''\n",
    "            seen_dois.add(doi)\n",
    "            return doi\n",
    "    \n",
    "        content = doi_pattern.sub(replace_doi, content)\n",
    "    \n",
    "        return content, content\n",
    "\n",
    "    \n",
    "###########################\n",
    "    #For fixed_size\n",
    "    \n",
    "    def create_chunks(self, text: str, metadata: Dict, chunk_type: str = \"character\") -> List[Document]:\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        # Enhanced text cleaning - do this FIRST before any processing\n",
    "        cleaned_text = self.clean_content(text)\n",
    "        #if not cleaned_text or len(cleaned_text) < self.MIN_CHUNK_CHARS:\n",
    "        if not cleaned_text: #softenning the upper condition    \n",
    "            return []\n",
    "\n",
    "    \n",
    "        def split_text_by_size(cleaned_text: str, max_size: int, overlap_size: int, min_chars: int) -> List[str]:\n",
    "            \"\"\"\n",
    "            Improved text splitting function that handles sentence boundaries and overlaps better.\n",
    "            \"\"\"            \n",
    "            if len(cleaned_text) <= max_size:\n",
    "                return [cleaned_text]\n",
    "            \n",
    "            chunks = []\n",
    "            start = 0\n",
    "            min_chunk_size = max(max_size // 2, max_size - overlap_size)\n",
    "            text_len = len(cleaned_text)  # Cache length\n",
    "            \n",
    "            while start < text_len:\n",
    "                end = min(start + max_size, text_len)\n",
    "                \n",
    "                # If this isn't the last chunk, try to find a good break point\n",
    "                if end < text_len:\n",
    "                    # Look for sentence endings in the last portion of the chunk\n",
    "                    #search_start = max(end - 200, start + min_chunk_size)\n",
    "                    search_start = max(end - int(max_size * 0.3), start + min_chunk_size)\n",
    "                    search_zone = cleaned_text[search_start:end]\n",
    "\n",
    "                    #Sara\n",
    "                    # # Find sentence boundaries (compiled regex could be cached as class attribute)\n",
    "                    # sentence_pattern = r'[.!?]\\s+|[.!?]$|\\n\\s*\\n'\n",
    "                    # matches = list(re.finditer(sentence_pattern, search_zone))\n",
    "\n",
    "                    sentence_pattern = re.compile(r'[.!?]\\s+|[.!?]$|\\n\\s*\\n')\n",
    "                    matches = list(sentence_pattern.finditer(search_zone))                    \n",
    "                    \n",
    "                    if matches:\n",
    "                        # Use the last sentence boundary found\n",
    "                        match = matches[-1]\n",
    "                        match_end = search_start + match.end()\n",
    "                        end = match_end - len(match.group().lstrip('.!?'))\n",
    "                    else:\n",
    "                        # Fall back to word boundary\n",
    "                        word_boundary = cleaned_text.rfind(' ', search_start, end)\n",
    "                        if word_boundary > start:\n",
    "                            end = word_boundary\n",
    "                \n",
    "                # Extract and validate chunk\n",
    "                chunk = cleaned_text[start:end].strip()\n",
    "                if len(chunk) >= min_chars:\n",
    "                    chunks.append(chunk)\n",
    "                \n",
    "                # Check if we've reached the end\n",
    "                if end >= text_len:\n",
    "                    break\n",
    "                \n",
    "                # Calculate next start position with proper overlap\n",
    "                ideal_next_start = end - overlap_size\n",
    "                \n",
    "                if ideal_next_start <= start:\n",
    "                    next_start = start + min_chunk_size\n",
    "                else:\n",
    "                    word_start = cleaned_text.find(' ', ideal_next_start)\n",
    "                    next_start = word_start + 1 if word_start != -1 and word_start < end else ideal_next_start\n",
    "                \n",
    "                start = next_start\n",
    "            \n",
    "            return chunks\n",
    "        \n",
    "        # Split text using instance variables directly\n",
    "        texts = split_text_by_size(text, self.chunk_size, self.chunk_overlap, self.MIN_CHUNK_CHARS)\n",
    "        \n",
    "        # Pre-compile commonly used values\n",
    "        section_title = metadata.get(\"section_title\", \"\")\n",
    "        section_level = metadata.get(\"section_level\", 0)\n",
    "        total_chunks = len(texts)\n",
    "        \n",
    "        unique_chunks = []\n",
    "        chunk_fingerprints = set()\n",
    "        \n",
    "        for i, chunk in enumerate(texts):\n",
    "            # Create fingerprint once (avoid duplicate normalization)\n",
    "            normalized_text = re.sub(r'\\s+', ' ', chunk.strip().lower())\n",
    "            text_fingerprint = hashlib.md5((normalized_text + section_title).encode()).hexdigest()\n",
    "            \n",
    "            if text_fingerprint in chunk_fingerprints:\n",
    "                continue\n",
    "            \n",
    "            chunk_fingerprints.add(text_fingerprint)\n",
    "            \n",
    "            # Use original chunk for content hash (avoid re-encoding)\n",
    "            content_hash = hashlib.md5(chunk.encode()).hexdigest()\n",
    "            \n",
    "            if content_hash not in self.processed_hashes:\n",
    "                #self.processed_hashes.add(content_hash)\n",
    "                self.add_to_processed_hashes(content_hash)\n",
    "                \n",
    "                # Build metadata once\n",
    "                chunk_metadata = {\n",
    "                    **metadata,  # Spread operator is more efficient than copy()\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": total_chunks,\n",
    "                    \"chunk_type\": chunk_type,\n",
    "                    \"section_title\": section_title,\n",
    "                    \"section_level\": section_level\n",
    "                }\n",
    "                \n",
    "                if i > 0:\n",
    "                    chunk_metadata[\"continued\"] = True\n",
    "                \n",
    "                unique_chunks.append(Document(page_content=chunk, metadata=chunk_metadata))\n",
    "                # self.debug_mode= True\n",
    "                # if self.debug_mode:\n",
    "        if chunk_type == \"character\":\n",
    "            logger.critical(f\"Chunking result: {len(unique_chunks)} chunks for {metadata.get('source')} in create chunks function\")\n",
    "        \n",
    "        return unique_chunks\n",
    "\n",
    "    \n",
    "    def create_full_text_chunks(self, text: str, metadata: Dict, chunk_type: str = \"full_text\") -> List[Document]:\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        # Enhanced text cleaning - do this FIRST before any processing\n",
    "        cleaned_text = self.clean_content(text)\n",
    "        #logger.debug(f\"Cleaned text length: {len(cleaned_text)}\")\n",
    "        if not cleaned_text or len(cleaned_text) < self.MIN_CHUNK_CHARS:\n",
    "            #logger.debug(f\"Skipping due to short cleaned content (< {self.MIN_CHUNK_CHARS})\")\n",
    "            return []\n",
    "    \n",
    "        def split_text_for_full_coverage(t: str, max_size: int) -> List[str]:\n",
    "            \"\"\"\n",
    "            Simple text splitting that ensures NO content loss - just cuts at max_size\n",
    "            \"\"\"\n",
    "            if len(t) <= max_size:\n",
    "                return [t]\n",
    "            \n",
    "            chunks = []\n",
    "            start = 0\n",
    "            text_len = len(t)\n",
    "            \n",
    "            while start < text_len:\n",
    "                end = min(start + max_size, text_len)\n",
    "                chunk = t[start:end].strip()\n",
    "                if chunk:  # Only add non-empty chunks\n",
    "                    chunks.append(chunk)\n",
    "                start = end  # Next chunk starts exactly where this one ended\n",
    "            \n",
    "            return chunks\n",
    "        \n",
    "        # Split text using the simple function\n",
    "        texts = split_text_for_full_coverage(cleaned_text, self.chunk_size)\n",
    "        #logger.debug(f\"Split into {len(texts)} chunks after cleaning\")\n",
    "\n",
    "        \n",
    "        # Keep all the deduplication logic from original function\n",
    "        section_title = metadata.get(\"section_title\", \"\")\n",
    "        section_level = metadata.get(\"section_level\", 0)\n",
    "        total_chunks = len(texts)\n",
    "        \n",
    "        unique_chunks = []\n",
    "        chunk_fingerprints = set()\n",
    "        \n",
    "        for i, chunk in enumerate(texts):\n",
    "            # Create fingerprint once (avoid duplicate normalization)\n",
    "            normalized_text = re.sub(r'\\s+', ' ', chunk.strip().lower())\n",
    "            text_fingerprint = hashlib.md5((normalized_text + section_title).encode()).hexdigest()\n",
    "            \n",
    "            if text_fingerprint in chunk_fingerprints:\n",
    "                continue\n",
    "            \n",
    "            chunk_fingerprints.add(text_fingerprint)\n",
    "            \n",
    "            # Use original chunk for content hash (avoid re-encoding)\n",
    "            content_hash = hashlib.md5(chunk.encode()).hexdigest()\n",
    "\n",
    "            # New hash is defined\n",
    "            if content_hash not in self.full_text_hashes:\n",
    "                #self.add_to_processed_hashes(content_hash)\n",
    "                self.full_text_hashes.add(content_hash)\n",
    "                \n",
    "                # Build metadata once\n",
    "                chunk_metadata = {\n",
    "                    **metadata,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": total_chunks,\n",
    "                    \"chunk_type\": \"full_text\",  # Force full_text type\n",
    "                    \"section_title\": section_title,\n",
    "                    \"section_level\": section_level\n",
    "                }\n",
    "                \n",
    "                if i > 0:\n",
    "                    chunk_metadata[\"continued\"] = True\n",
    "                \n",
    "                unique_chunks.append(Document(page_content=chunk, metadata=chunk_metadata))\n",
    "        \n",
    "        return unique_chunks\n",
    "            \n",
    "    \n",
    "# Final        \n",
    "    def create_structure_based_chunks(self, soup: BeautifulSoup, url: str, depth: int, language: str) -> List[Document]:\n",
    "        if self.is_login_page(soup) or self.is_not_found_page(soup):\n",
    "            logger.warning(f\"Skipping {url}: Login or not found page\")\n",
    "            return []\n",
    "        \n",
    "        chunks = []\n",
    "        processed_elements = set()\n",
    "        #detected_language = self.detect_language(soup, soup.get_text(strip=True), url)\n",
    "        page_title = soup.title.text.strip() if soup.title else \"No title\"\n",
    "        #logger.info(f\"Processing URL: {url}, Title: {page_title}\")\n",
    "        logger.info(f\"Processing URL: {url}\")\n",
    "        \n",
    "        # Define header tags for hierarchical processing\n",
    "        header_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
    "        \n",
    "        def add_section_to_chunks(section: Dict, metadata: Dict) -> List[Document]:\n",
    "            if not section[\"content\"]:\n",
    "                return []\n",
    "            content_text = \"\\n\".join(section[\"content\"]).strip()\n",
    "            full_text = f\"{section['title']}\\n\\n{content_text}\" if section[\"title\"] else content_text\n",
    "            if len(full_text) < self.MIN_CHUNK_CHARS:\n",
    "                return []\n",
    "                \n",
    "    # Use extract_content to align with Code 2's cleaning and deduplication\n",
    "            cleaned_content, _ = self.extract_content(BeautifulSoup(full_text, 'html.parser'), use_tags=False)\n",
    "            #cleaned_content = self.clean_content(full_text)\n",
    "\n",
    "            if not cleaned_content:\n",
    "                return []\n",
    "            chunks = self.create_chunks(cleaned_content, metadata, chunk_type=\"structural\")\n",
    "            return [chunk for chunk in chunks if len(chunk.page_content) >= self.MIN_CHUNK_CHARS]\n",
    "            \n",
    "            # chunks = self.create_chunks(full_text, metadata, chunk_type=\"structural\")  # Cleaning done in create_chunks\n",
    "            # return [chunk for chunk in chunks if len(chunk.page_content) >= self.MIN_CHUNK_CHARS]\n",
    "        \n",
    "        # Process semantic sections\n",
    "        section_tags = [\n",
    "            \"section\", \"article\", \"main\", \"div.content-section\", \"div.module\", \"div.text\",\n",
    "            \"div.content\", \"div.text-block\", \"div.main-content\", \"div.container\", \"div.row\",\n",
    "            \"div.card\", \"div.content-main\", \"div.teaser-text\", \"div.publication-item\",\n",
    "            \"div.news-item\", \"div.portlet-body\", \"div.event-details\", \"div.indico-content\",\n",
    "            \"div.publication-list\", \"div.event-description\", \"div.news-content\",\n",
    "            \"div.status-report\", \"div.status\", \"div.monitor\", \"div.experiment\", \"div.results\",\n",
    "            \"div.timetable\", \"p\", \"p[id]\", \"span\", \"table\", \"table.i-table\", \"caption\",\n",
    "            \"td\", \"th\", \"tr\", \"ul\", \"ol\", \"li\", *header_tags\n",
    "        ]\n",
    "        \n",
    "        # Try semantic sections first\n",
    "        for tag in section_tags:\n",
    "            if \".\" in tag:\n",
    "                tag_name, tag_class = tag.split(\".\", 1)\n",
    "                elements = soup.find_all(tag_name, class_=tag_class)\n",
    "            elif tag.startswith('p['):\n",
    "                elements = soup.find_all('p', id=True)\n",
    "            else:\n",
    "                elements = soup.find_all(tag)\n",
    "            for element in elements:\n",
    "                # Skip if this element or its parent was already processed\n",
    "                if id(element) in processed_elements:\n",
    "                    continue\n",
    "                \n",
    "                # Skip if any ancestor was already processed\n",
    "                if any(id(ancestor) in processed_elements for ancestor in element.parents):\n",
    "                    continue\n",
    "      \n",
    "                text = element.get_text(separator=\" \", strip=True)\n",
    "                if text and len(text) > self.MIN_INITIAL_CHARS:\n",
    "                    processed_elements.add(id(element))\n",
    "                        # Also mark all descendants as processed\n",
    "                    for descendant in element.descendants:\n",
    "                        if hasattr(descendant, 'name'):\n",
    "                            processed_elements.add(id(descendant))\n",
    "                            \n",
    "                        metadata = {\n",
    "                            \"source\": url,\n",
    "                            \"section_title\": element.find(header_tags).get_text(strip=True)\n",
    "                                             if element.find(header_tags) else page_title,\n",
    "                            \"section_level\": 1,\n",
    "                            \"language\": language,\n",
    "                            \"depth\": depth,\n",
    "                            \"title\": page_title\n",
    "                        }\n",
    "                    # Collect text and defer chunking to add_section_to_chunks\n",
    "                    section = {\"title\": metadata[\"section_title\"], \"content\": [text], \"level\": 1}\n",
    "                    chunks.extend(add_section_to_chunks(section, metadata))\n",
    "                \n",
    "                    #chunks.extend(self.create_chunks(text, metadata, chunk_type=\"structural\"))\n",
    "        \n",
    "        # Hierarchical processing for headers and content\n",
    "        if not chunks:\n",
    "            #logger.debug(f\"No chunks from semantic sections for {url}, trying hierarchical processing\")\n",
    "            active_sections = {}\n",
    "            elements = soup.find_all([*header_tags, 'p', 'li', 'td'])\n",
    "            for element in elements:\n",
    "                tag_name = element.name\n",
    "                if tag_name in header_tags:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    level = int(tag_name[1])\n",
    "                    for i in range(level, 7):\n",
    "                        if i in active_sections:\n",
    "                            metadata = {\n",
    "                                \"source\": url,\n",
    "                                \"section_title\": active_sections[i][\"title\"],\n",
    "                                \"section_level\": active_sections[i][\"level\"],\n",
    "                                \"language\": language,\n",
    "                                \"depth\": depth,\n",
    "                                \"title\": page_title\n",
    "                            }\n",
    "                            chunks.extend(add_section_to_chunks(active_sections[i], metadata))\n",
    "                            del active_sections[i]\n",
    "                    active_sections[level] = {\"title\": text, \"content\": [], \"level\": level}\n",
    "                else:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    if text and active_sections:\n",
    "                        active_sections[max(active_sections.keys())][\"content\"].append(text)\n",
    "            for level in sorted(active_sections.keys()):\n",
    "                metadata = {\n",
    "                    \"source\": url,\n",
    "                    \"section_title\": active_sections[level][\"title\"],\n",
    "                    \"section_level\": active_sections[level][\"level\"],\n",
    "                    \"language\": language,\n",
    "                    \"depth\": depth,\n",
    "                    \"title\": page_title\n",
    "                }\n",
    "                chunks.extend(add_section_to_chunks(active_sections[level], metadata))\n",
    "        \n",
    "        # Fallback to body\n",
    "        if not chunks:\n",
    "            #logger.warning(f\"No chunks from semantic or hierarchical processing for {url}, falling back to body\")\n",
    "            body = soup.find('body') or soup\n",
    "            body_text = body.get_text(separator=\" \", strip=True)\n",
    "            if body_text:\n",
    "                metadata = {\n",
    "                    \"source\": url,\n",
    "                    \"section_title\": page_title,\n",
    "                    \"section_level\": 0,\n",
    "                    \"language\": language,\n",
    "                    \"depth\": depth,\n",
    "                    \"title\": page_title\n",
    "                }\n",
    "                chunks.extend(self.create_chunks(body_text, metadata, chunk_type=\"structural\"))\n",
    "                \n",
    "        logger.critical(f\"Chunking result: {len(chunks)} chunks for {url} in create STRUCTURAL chunks function\")\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    async def create_session(self, url: str = None) -> aiohttp.ClientSession:\n",
    "        async with self.session_lock:\n",
    "            if not hasattr(self, 'session_request_count'):\n",
    "                self.session_request_count = 0\n",
    "    \n",
    "            if self.session and not self.session.closed and self.session_request_count > 50:\n",
    "                await self.session.close()\n",
    "                self.session = None\n",
    "                self.session_request_count = 0\n",
    "                await asyncio.sleep(0.1)\n",
    "    \n",
    "            if self.session and not self.session.closed:\n",
    "                #logger.info(f\"[Session] Current count: {self.session_request_count} | Session closed: False\")\n",
    "                return self.session\n",
    "    \n",
    "            domain = urlparse(url).netloc if url else None\n",
    "            domain_config = self.domain_configs.get(domain, self.default_domain_config)\n",
    "            ssl_context = ssl.create_default_context()\n",
    "            ssl_context.check_hostname = False\n",
    "            ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "            # Rotate User-Agent\n",
    "            user_agents = [\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/125.0.0.0 Safari/537.36',\n",
    "                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Version/14.0 Safari/605.1.15',\n",
    "                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36'\n",
    "            ]\n",
    "        \n",
    "    \n",
    "            self.session = aiohttp.ClientSession(\n",
    "                connector=aiohttp.TCPConnector(\n",
    "                    limit=domain_config.get(\"max_connections\", 10),\n",
    "                    ttl_dns_cache=300,\n",
    "                    ssl=ssl_context,\n",
    "                    force_close=False,\n",
    "                    enable_cleanup_closed=True,\n",
    "                    resolver=aiohttp.AsyncResolver()\n",
    "                ),\n",
    "                timeout=aiohttp.ClientTimeout(total=self.timeout, connect=10, sock_read=20),\n",
    "                headers={\n",
    "                    'User-Agent': random.choice(user_agents), \n",
    "                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                    'Accept-Language': 'en-US,en;q=0.5',\n",
    "                    'Connection': 'keep-alive'\n",
    "                }\n",
    "            )\n",
    "            return self.session\n",
    "    \n",
    "\n",
    "    \n",
    "    async def close_session(self):\n",
    "        \"\"\"Close the aiohttp session.\"\"\"\n",
    "        if self.session and not self.session.closed:\n",
    "            await self.session.close()\n",
    "            self.session = None\n",
    "            logger.info(\"Session closed successfully\")\n",
    "\n",
    "    \n",
    "    \n",
    "    async def fetch_simple(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Fetch URL content using aiohttp.\"\"\"\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                parsed_url = urlparse(url)\n",
    "                domain = parsed_url.netloc\n",
    "                domain_config = self.domain_configs.get(domain, self.default_domain_config)\n",
    "                session = await self.create_session(url)\n",
    "               # headers = {'Referer': f\"{parsed_url.scheme}://{domain}\"}\n",
    "          \n",
    "                headers = {\n",
    "                    \"Referer\": random.choice([\n",
    "                        \"https://google.com\",\n",
    "                        \"https://duckduckgo.com\",\n",
    "                        \"https://www.bing.com\",\n",
    "                        f\"https://{parsed_url.netloc}/\",\n",
    "                        \"https://twitter.com\",\n",
    "                        \"https://facebook.com\"\n",
    "                    ]),\n",
    "                    \"Origin\": f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "                }\n",
    "\n",
    "                timeout_config = aiohttp.ClientTimeout(\n",
    "                    total=domain_config.get(\"timeout\", self.timeout), #how long the full request can take\n",
    "                    connect=60, #how long to wait to connect to the server\n",
    "                    sock_connect=30, #30 second time to open the socket -30\n",
    "                    sock_read=120 # 120 second time to read data from the server\n",
    "                )\n",
    "                # I Added small delay to mimic human behavior\n",
    "                await asyncio.sleep(random.uniform(0.5, 1.5)) #(0.3, 0.8)) #0.5, 1.5\n",
    "                \n",
    "                async with session.get(\n",
    "                    url,\n",
    "                    headers=headers,\n",
    "                    allow_redirects=True,\n",
    "                    timeout=timeout_config\n",
    "                ) as response:\n",
    "                    self.session_request_count += 1\n",
    "                    \n",
    "                    if response.status == 200:\n",
    "                        resolved_url = str(response.url)\n",
    "                        text = await response.text(errors='replace')\n",
    "    \n",
    "                        # Soft block detection\n",
    "                        if len(text.strip()) < 500 or 'access denied' in text.lower() or 'javascript required' in text.lower():\n",
    "                            logger.warning(f\"[Soft block?] Suspiciously low content from {url}\")\n",
    "                            logger.debug(f\"[{url}] Response preview: {text.strip()[:300]}\")  \n",
    "                            self.error_urls[url] = \"Soft block or empty content\"\n",
    "    \n",
    "                            # Optional fallback to JS rendering\n",
    "                            if hasattr(self, 'fetch_with_js'):\n",
    "                                logger.info(f\"Falling back to JS rendering for {url}\")\n",
    "                                return await self.fetch_with_js(url)\n",
    "                            return None\n",
    "\n",
    "\n",
    "                        \n",
    "                        if resolved_url != url:\n",
    "                            logger.info(f\"Redirected {url} to {resolved_url}\")\n",
    "                            self.redirected_urls[url] = resolved_url  # Track redirect\n",
    "                            if resolved_url not in self.processed_urls:\n",
    "                                return await response.text(errors='replace')\n",
    "                            else:\n",
    "                                self.add_to_processed_urls(url)    \n",
    "                                return None\n",
    "                        else:\n",
    "                            return await response.text(errors='replace')\n",
    "                    else:\n",
    "                        self.error_urls[url] = f\"HTTP status: {response.status}\"\n",
    "                        return None\n",
    "        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"[Attempt {attempt+1}] Error fetching {url}: {e}\")\n",
    "                if attempt == 2:\n",
    "                    self.error_urls[url] = f\"{str(e)}\"\n",
    "                    return None\n",
    "                await asyncio.sleep(2 ** attempt)  # exponential backoff\n",
    "\n",
    "    async def init_browser(self):\n",
    "        async with self.browser_lock:\n",
    "            if self.browser is None or self.context is None:\n",
    "                self.playwright = await async_playwright().start()\n",
    "                self.browser = await self.playwright.chromium.launch(\n",
    "                    headless=True,\n",
    "                    args=[\n",
    "                        \"--no-sandbox\",\n",
    "                        \"--disable-setuid-sandbox\",\n",
    "                        \"--disable-gpu-vsync\"\n",
    "                    ]\n",
    "                )\n",
    "    \n",
    "                user_agents = [\n",
    "                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/125.0.0.0 Safari/537.36',\n",
    "                    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Version/14.0 Safari/605.1.15',\n",
    "                    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36'\n",
    "                ]\n",
    "                chosen_user_agent = random.choice(user_agents)\n",
    "                \n",
    "\n",
    "            \n",
    "    async def fetch_with_js(self, url: str) -> Optional[str]:\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                async with self.js_semaphore:  # 🔒 Limit concurrent JS fetches\n",
    "                    if self.browser is None:\n",
    "                        await self.init_browser()\n",
    "    \n",
    "                    # 🔄 NEW: Create a fresh browser context for this task\n",
    "                    user_agents = [\n",
    "                        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/125.0.0.0 Safari/537.36',\n",
    "                        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Version/14.0 Safari/605.1.15',\n",
    "                        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36'\n",
    "                    ]\n",
    "                    chosen_user_agent = random.choice(user_agents)\n",
    "\n",
    "                    referers = [\n",
    "                        \"https://google.com\",\n",
    "                        # \"https://duckduckgo.com\",\n",
    "                        # \"https://www.bing.com\",\n",
    "                        f\"https://{urlparse(url).netloc}/\",\n",
    "                        # \"https://twitter.com\",\n",
    "                        # \"https://facebook.com\"\n",
    "                        f\"https://{urlparse(url).netloc}/index.html\",\n",
    "                        \"https://www.desy.de/\",\n",
    "                        \"https://desy.de/\"\n",
    "                    ]\n",
    "                    chosen_referer = random.choice(referers)\n",
    "                    \n",
    "                    # ✅ Step 1: Build headers dictionary\n",
    "                    extra_headers = {\n",
    "                        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "                        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "                        \"Connection\": \"keep-alive\",\n",
    "                        \"Referer\": chosen_referer,\n",
    "                        \"Origin\": f\"{urlparse(url).scheme}://{urlparse(url).netloc}\"\n",
    "                    }\n",
    "                    \n",
    "                    # ✅ Step 2: Add optional stealth headers\n",
    "                    extra_headers.update({\n",
    "                        \"DNT\": \"1\",\n",
    "                        \"Sec-Fetch-Site\": \"cross-site\",\n",
    "                        #\"Sec-Fetch-Site\": \"same-origin\",\n",
    "                        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "                        \"Sec-Fetch-Dest\": \"document\",\n",
    "                        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "                    })\n",
    "                    \n",
    "                    # ✅ Step 3: Pass the complete headers\n",
    "                    context = await self.browser.new_context(\n",
    "                        user_agent=chosen_user_agent,\n",
    "                        locale=\"en-US\",\n",
    "                        viewport={\"width\": 1280, \"height\": 800},\n",
    "                        extra_http_headers=extra_headers,\n",
    "                        ignore_https_errors=True  # ✅ Enables scraping of HTTP/HTTPS-mismatched pages\n",
    "                    )\n",
    "\n",
    "    \n",
    "                    # ✅ NEW: Open a page from the fresh context\n",
    "                    page = await context.new_page()\n",
    "    \n",
    "                    domain = urlparse(url).netloc\n",
    "                    domain_config = self.domain_configs.get(domain, self.default_domain_config)\n",
    "                    timeout_ms = domain_config.get(\"timeout\", self.timeout) * 1000\n",
    "                    js_wait_time = domain_config.get(\"js_wait_time\", self.js_wait_time)\n",
    "                    consent_timeout = domain_config.get(\"consent_timeout\", 300) # 300- 5000 #sara\n",
    "    \n",
    "                    try:\n",
    "                        response = await page.goto(url, wait_until='networkidle', timeout=timeout_ms)\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            response = await page.goto(url, wait_until='domcontentloaded', timeout=timeout_ms)\n",
    "                        except Exception:\n",
    "                            response = await page.goto(url, timeout=timeout_ms // 2)\n",
    "\n",
    "                    # ✅ Insert this check right after the response is obtained\n",
    "                    if response and not response.ok:\n",
    "                        logger.warning(f\"[{url}] JS response status: {response.status}\")\n",
    "    \n",
    "    \n",
    "                    if response:\n",
    "                        final_url = page.url\n",
    "                        if final_url != url:\n",
    "                            logger.info(f\"JS redirected {url} to {final_url}\")\n",
    "                            async with self.url_lock:\n",
    "                                self.redirected_urls[url] = final_url\n",
    "                                if final_url in self.processed_urls:\n",
    "                                    self.add_to_processed_urls(url)\n",
    "                                    await page.close()\n",
    "                                    await context.close()\n",
    "                                    return None\n",
    "    \n",
    "                        try:\n",
    "                            \n",
    "\n",
    "                            await page.click(\n",
    "                                'button:has-text(\"Accept\"), a:has-text(\"OK\"), div:has-text(\" Agree\"), '\n",
    "                                'button:has-text(\"Consent\"), button:has-text(\"Zustimmen\")',\n",
    "                                timeout=consent_timeout\n",
    "                            )\n",
    "                        except Exception:\n",
    "                            pass\n",
    "    \n",
    "                        await asyncio.sleep(js_wait_time / 1000)\n",
    "    \n",
    "                        if self.js_scroll:\n",
    "                            await self._scroll_page(page)\n",
    "    \n",
    "                        content = await page.content()\n",
    "    \n",
    "                        if len(content) > 5_000_000:\n",
    "                            logger.warning(f\"==================Page content from {url} too large to process safely\")\n",
    "                            await page.close()\n",
    "                            await context.close()\n",
    "                            return None\n",
    "    \n",
    "                        soup = BeautifulSoup(content, 'html.parser')\n",
    "                        soup.resolved_url = page.url\n",
    "    \n",
    "                        if 'login' in page.url.lower() or 'auth' in page.url.lower():\n",
    "                            async with self.url_lock:\n",
    "                                self.error_urls[url] = f\"Redirected to login page: {page.url}\"\n",
    "                            await page.close()\n",
    "                            await context.close()\n",
    "                            return None\n",
    "    \n",
    "                        if len(soup.get_text(strip=True)) >= 100 and len(soup.find_all(['p', 'div', 'section'])) >= 5:\n",
    "                            await page.close()\n",
    "                            await context.close()\n",
    "                            return content\n",
    "    \n",
    "                        logger.warning(f\"Low content density in JS render for {url}, attempt {attempt + 1}\")\n",
    "                        await page.close()\n",
    "                        await context.close()\n",
    "    \n",
    "                    await asyncio.sleep(2)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"[Attempt {attempt + 1}] JS error on {url}: {e}\")\n",
    "                if attempt == 2:\n",
    "                    async with self.url_lock:\n",
    "                        self.error_urls[url] = f\"JS rendering failed: {str(e)}\"\n",
    "                    return None\n",
    "                await asyncio.sleep(2 ** attempt)\n",
    "\n",
    "\n",
    "    \n",
    "    async def _scroll_page(self, page):\n",
    "        \"\"\"Scroll page to load lazy-loaded content.\"\"\"\n",
    "        try:\n",
    "            height = await page.evaluate('document.body.scrollHeight')\n",
    "            for i in range(0, height, 300):\n",
    "                await page.evaluate(f'window.scrollTo(0, {i})')\n",
    "                await asyncio.sleep(0.1)\n",
    "            await page.evaluate('window.scrollTo(0, 0)')\n",
    "            await asyncio.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scrolling page: {str(e)}\")\n",
    "\n",
    "\n",
    "    async def debug_character_extraction(self, url: str, soup: BeautifulSoup) -> Dict:\n",
    "        \"\"\"Debug helper to understand why character extraction fails\"\"\"\n",
    "        debug_info = {\n",
    "            'url': url,\n",
    "            'has_soup': soup is not None,\n",
    "            'title': soup.title.text.strip() if soup and soup.title else \"No title\",\n",
    "            'total_text_length': len(soup.get_text(strip=True)) if soup else 0,\n",
    "            'tag_extraction_results': {},\n",
    "            'fallback_results': {}\n",
    "        }\n",
    "        \n",
    "        if not soup:\n",
    "            debug_info['error'] = \"No soup object\"\n",
    "            return debug_info\n",
    "\n",
    "        for tag in self.content_tags[:5]:  # Test first 5 tags\n",
    "            if \".\" in tag:\n",
    "                tag_name, tag_class = tag.split(\".\")\n",
    "                elements = soup.find_all(tag_name, class_=tag_class)\n",
    "            else:\n",
    "                elements = soup.find_all(tag)\n",
    "            \n",
    "            tag_text_length = sum(len(elem.get_text(strip=True)) for elem in elements)\n",
    "            debug_info['tag_extraction_results'][tag] = {\n",
    "                'elements_found': len(elements),\n",
    "                'total_text_length': tag_text_length\n",
    "            }\n",
    "        main_content = soup.find('main') or soup.find('body') or soup\n",
    "        debug_info['fallback_results'] = {\n",
    "            'main_content_tag': main_content.name if main_content else None,\n",
    "            'main_content_length': len(main_content.get_text(strip=True)) if main_content else 0\n",
    "        }\n",
    "        \n",
    "        return debug_info\n",
    "\n",
    "    \n",
    "    async def fetch_with_retry(self, fetch_function, url: str, max_retries: int = 3) -> Optional[str]:\n",
    "        \"\"\"Retry fetching with exponential backoff.\"\"\"\n",
    "        domain = urlparse(url).netloc\n",
    "        base_delay = self.domain_configs.get(domain, self.default_domain_config).get(\"retry_delay\", 2)\n",
    "        for attempt in range(max_retries):\n",
    "            if attempt > 0:\n",
    "                delay = base_delay * (2 ** (attempt - 1)) * (0.5 + random.random())\n",
    "                await asyncio.sleep(delay)\n",
    "            try:\n",
    "                return await fetch_function(url)\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    self.error_urls[url] = str(e)\n",
    "                    return None\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    async def fetch_url_async(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        if self.should_skip_url(url):\n",
    "            async with self.url_lock:\n",
    "                self.error_urls[url] = \"Non-HTML content (skipped by extension)\"\n",
    "            return None\n",
    "    \n",
    "        content = await self.fetch_simple(url)\n",
    "        if content:\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "            if self.is_login_page(soup):\n",
    "                logger.warning(f\"[{url}] Skipping login page\")\n",
    "                self.error_urls[url] = \"Login page detected\"\n",
    "                return None\n",
    "    \n",
    "            if self.is_not_found_page(soup):\n",
    "                logger.warning(f\"[{url}] Skipping not-found page\")\n",
    "                self.error_urls[url] = \"404 or content error detected\"\n",
    "                return None\n",
    "    \n",
    "            text_content = soup.get_text(strip=True)\n",
    "            structure_tags = soup.find_all(['p', 'div', 'section', 'article'])\n",
    "    \n",
    "            # ✅ Dynamic JS dependency check\n",
    "            weak_text = len(text_content) < 200\n",
    "            low_structure = len(structure_tags) < 5\n",
    "            #few_links = len(soup.find_all('a')) < 3\n",
    "            has_js_warning = \"javascript required\" in content.lower()\n",
    "    \n",
    "            scripts = soup.find_all(\"script\", src=True)\n",
    "            external_js_count = sum(1 for s in scripts if 'zmi.js' in s['src'] or s['src'].startswith(\"/++resource++\"))\n",
    "            has_noscript_warning = bool(soup.find(\"noscript\"))\n",
    "            js_suspect = external_js_count > 1 or has_noscript_warning\n",
    "    \n",
    "            # ✅ Logging useful debug signals for diagnostics\n",
    "            #logger.debug(f\"[{url}] Text length: {len(text_content)}, Tags: {len(structure_tags)}, Links: {len(soup.find_all('a'))}, Scripts: {external_js_count}\")\n",
    "    \n",
    "            if weak_text or low_structure or has_js_warning or js_suspect: #or few_links\n",
    "                logger.info(f\"[{url}] HTML signal suggests possible JS-dependency — triggering JS fallback\")\n",
    "                js_content = await self.fetch_with_js(url)\n",
    "    \n",
    "                if js_content:\n",
    "                    soup = BeautifulSoup(js_content, \"html.parser\")\n",
    "    \n",
    "                    if self.is_login_page(soup):\n",
    "                        logger.warning(f\"[{url}] JS fallback landed on login page\")\n",
    "                        self.error_urls[url] = \"Login page detected (post-JS)\"\n",
    "                        return None\n",
    "    \n",
    "                    if self.is_not_found_page(soup):\n",
    "                        logger.warning(f\"[{url}] JS fallback returned not-found page\")\n",
    "                        self.error_urls[url] = \"404 (post-JS)\"\n",
    "                        return None\n",
    "    \n",
    "                    return soup\n",
    "    \n",
    "                return None\n",
    "    \n",
    "            return soup\n",
    "    \n",
    "        # If fetch_simple failed entirely, try JS rendering\n",
    "        js_content = await self.fetch_with_js(url)\n",
    "        if js_content:\n",
    "            soup = BeautifulSoup(js_content, \"html.parser\")\n",
    "    \n",
    "            if self.is_login_page(soup):\n",
    "                logger.warning(f\"[{url}] JS-only fallback landed on login page\")\n",
    "                self.error_urls[url] = \"Login page detected (pure JS)\"\n",
    "                return None\n",
    "    \n",
    "            if self.is_not_found_page(soup):\n",
    "                logger.warning(f\"[{url}] JS-only fallback returned not-found page\")\n",
    "                self.error_urls[url] = \"404 (pure JS)\"\n",
    "                return None\n",
    "    \n",
    "            return soup\n",
    "    \n",
    "        return None\n",
    "    \n",
    "        \n",
    "    \n",
    "        \n",
    "    async def process_url(self, url: str, depth: int) -> Tuple[List[Document], List[Document], List[Document]]:\n",
    "        \"\"\"Enhanced process_url with debugging for character extraction failures\"\"\"\n",
    "\n",
    "        # ✅ Skip if already processed and cached\n",
    "        if url in self.processed_urls and url in self.url_to_documents_map:\n",
    "            cached_docs = self.url_to_documents_map[url]\n",
    "\n",
    "            char_docs = [doc for doc in cached_docs if doc.metadata[\"chunk_type\"] == \"character\"]\n",
    "            struct_docs = [doc for doc in cached_docs if doc.metadata[\"chunk_type\"] == \"structural\"]\n",
    "            full_docs = [doc for doc in cached_docs if doc.metadata[\"chunk_type\"] == \"full_text\"]\n",
    "            return char_docs, struct_docs, full_docs\n",
    "\n",
    "        \n",
    "    \n",
    "        # ✅ Skip if URL has already been redirected and processed\n",
    "        if url in self.redirected_urls and self.redirected_urls[url] in self.processed_urls:\n",
    "            logger.info(f\"Skipping {url}: Redirected to {self.redirected_urls[url]}, which is already processed.\")\n",
    "            self.add_to_processed_urls(url)\n",
    "            return [], [], []\n",
    "            \n",
    "    \n",
    "        soup = await self.fetch_with_retry(self.fetch_url_async, url)\n",
    "        if not soup or self.is_login_page(soup) or self.is_not_found_page(soup):\n",
    "                self.error_urls[url] = \"Failed to fetch or invalid page\"\n",
    "                self.add_to_processed_urls(url)  \n",
    "                return [], [], []\n",
    "\n",
    "        # **ADD THIS: Clean the HTML structure before content extraction**\n",
    "        cleaned_soup = soup\n",
    "    \n",
    "        #title = soup.title.text.strip() if soup.title else \"No title\"\n",
    "        title = cleaned_soup.title.text.strip() if cleaned_soup.title else \"No title\"\n",
    "        content, text_sample = self.extract_content(cleaned_soup, use_tags=True)\n",
    "        #content = self.clean_content(content)#Sara\n",
    "        detected_language = self.detect_language(cleaned_soup, text_sample, url)\n",
    "        self.track_page_character_count(url, content, title=title, language=detected_language, depth=depth)\n",
    "    \n",
    "        # Character-based chunks\n",
    "        char_docs = []\n",
    "        if len(content) >= self.MIN_CHUNK_CHARS:\n",
    "            char_metadata = {\n",
    "                \"source\": url,\n",
    "                \"title\": title,\n",
    "                \"depth\": depth,\n",
    "                \"language\": detected_language\n",
    "            }\n",
    "            char_docs = self.create_chunks(content, char_metadata, chunk_type=\"character\")\n",
    "\n",
    "        # Structural chunks\n",
    "        struct_docs = self.create_structure_based_chunks(cleaned_soup, url, depth, detected_language)\n",
    "\n",
    "        # # Full-text document\n",
    "        full_docs = []\n",
    "        \n",
    "        if len(content) >= self.MIN_CHUNK_CHARS:\n",
    "            full_metadata = {\n",
    "                \"source\": url,\n",
    "                \"title\": title,\n",
    "                \"depth\": depth,\n",
    "                \"language\": detected_language,\n",
    "                \"chunk_type\": \"full_text\"\n",
    "            }\n",
    "            #full_docs = self.create_full_text_chunks(content, full_metadata, chunk_type=\"full_text\")\n",
    "            content_hash = hashlib.md5(content.encode()).hexdigest()\n",
    "            self.add_to_processed_hashes(content_hash)\n",
    "            full_docs = [Document(page_content=content, metadata=full_metadata)]\n",
    "\n",
    "\n",
    "        self.track_extraction_results(\n",
    "            url,\n",
    "            character_success=bool(char_docs),\n",
    "            structural_success=bool(struct_docs),\n",
    "            character_count=len(char_docs),\n",
    "            structural_count=len(struct_docs)\n",
    "        )\n",
    "        \n",
    "\n",
    "        all_docs = char_docs + struct_docs + full_docs\n",
    "        if all_docs:  # Only mark as processed if something was extracted\n",
    "            self.add_to_processed_urls(url)\n",
    "            self.url_to_documents_map[url] = all_docs\n",
    "        else:\n",
    "            if url not in self.redirected_urls:\n",
    "                logger.warning(f\"⚠️ No chunks extracted for {url} — skipping marking as processed\")\n",
    "            # else:\n",
    "            #     logger.info(f\"[{url}] No chunks extracted — but URL was redirected to {self.redirected_urls[url]}\")\n",
    "            \n",
    "        \n",
    "        return char_docs, struct_docs, full_docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_character_counts(self, final: bool = False):\n",
    "        \"\"\"Save character counts to JSON file.\"\"\"\n",
    "        try:\n",
    "            filename = \"page_character_counts_final.json\" if final else f\"page_character_counts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            \n",
    "            # Create summary statistics\n",
    "            total_pages = len(self.page_character_counts)\n",
    "            total_characters = sum(page['character_count'] for page in self.page_character_counts.values())\n",
    "            total_words = sum(page['word_count'] for page in self.page_character_counts.values())\n",
    "            avg_chars_per_page = total_characters / total_pages if total_pages > 0 else 0\n",
    "            \n",
    "            # Group by language\n",
    "            language_stats = {}\n",
    "            for page in self.page_character_counts.values():\n",
    "                lang = page['language']\n",
    "                if lang not in language_stats:\n",
    "                    language_stats[lang] = {'pages': 0, 'characters': 0, 'words': 0}\n",
    "                language_stats[lang]['pages'] += 1\n",
    "                language_stats[lang]['characters'] += page['character_count']\n",
    "                language_stats[lang]['words'] += page['word_count']\n",
    "            \n",
    "            data = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'summary': {\n",
    "                    'total_pages': total_pages,\n",
    "                    'total_characters': total_characters,\n",
    "                    'total_words': total_words,\n",
    "                    'average_characters_per_page': round(avg_chars_per_page, 2),\n",
    "                    'language_breakdown': language_stats\n",
    "                },\n",
    "                'pages': list(self.page_character_counts.values())\n",
    "            }\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            logger.info(f\"Character counts saved to {filename}\")\n",
    "            logger.info(f\"Summary: {total_pages} pages, {total_characters:,} characters, {total_words:,} words\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving character counts: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "    def _save_progress(self, documents: List[Document], processed_urls: Set[str], error_urls: Dict[str, str], final: bool = False, chunk_type: str = \"character\"):\n",
    "        \"\"\"Save processing progress to disk.\"\"\"\n",
    "        try:\n",
    "            prefix = {\"character\": \"processor_sized_base\", \"structural\": \"processor_structural_base\", \"full_text\": \"processor_full_text_base\"}[chunk_type]\n",
    "            filename = f\"{prefix}_results_final.json\" if final else f\"{prefix}_progress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            data = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'processed_urls_count': len(processed_urls),\n",
    "                'documents_count': len(documents),\n",
    "                'error_urls_count': len(error_urls),\n",
    "                'processed_urls': list(processed_urls),\n",
    "                'error_urls': error_urls,\n",
    "                'document_metadata': [doc.metadata for doc in documents]\n",
    "            }\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            text_filename = f\"{prefix}_text_chunks_final.json\" if final else f\"{prefix}_text_chunks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            text_data = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'text_chunks': [{'content': doc.page_content, 'metadata': doc.metadata} for doc in documents]\n",
    "            }\n",
    "            with open(text_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(text_data, f, ensure_ascii=False, indent=2)\n",
    "            #logger.info(f\"Progress saved to {filename} and {text_filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving progress: {e}\")\n",
    "\n",
    "\n",
    "    async def process_urls_from_mapping(self, url_map_file: str, batch_size=None, limit=None) -> Dict[str, List[Document]]:\n",
    "        batch_size = batch_size or self.batch_size\n",
    "        \"\"\"Process URLs from a mapping file with all chunking strategies.\"\"\"\n",
    "        try:\n",
    "            with open(url_map_file, 'r', encoding='utf-8') as f:\n",
    "                url_map = json.load(f)\n",
    "    \n",
    "            urls_to_process = []\n",
    "            skipped_urls = []\n",
    "            for depth in range(self.max_depth + 1):\n",
    "                depth_key = str(depth)\n",
    "                if depth_key in url_map[\"urls_by_depth\"]:\n",
    "                    for url in url_map[\"urls_by_depth\"][depth_key]:\n",
    "                        if self.should_skip_url(url):\n",
    "                            skipped_urls.append(url)\n",
    "                        else:\n",
    "                            urls_to_process.append((url, depth))\n",
    "    \n",
    "            # Deduplicate URLs while preserving first occurrence\n",
    "            unique_urls = {}\n",
    "            for url, depth in urls_to_process:\n",
    "                if url not in unique_urls:\n",
    "                    unique_urls[url] = depth\n",
    "            unique_urls_to_process = list(unique_urls.items())\n",
    "    \n",
    "            if limit:\n",
    "                unique_urls_to_process = unique_urls_to_process[:limit]\n",
    "    \n",
    "            if skipped_urls:\n",
    "                logger.info(f\"Skipped {len(skipped_urls)} URLs with non-HTML extensions\")\n",
    "            logger.info(f\"Loaded {len(unique_urls_to_process)} unique URLs from mapping file\")\n",
    "    \n",
    "            self.progress_bar = tqdm(total=len(unique_urls_to_process), desc=\"Processing URLs\", unit=\"URL\", dynamic_ncols=True)\n",
    "            semaphore = asyncio.Semaphore(self.max_workers)\n",
    "            character_chunks, structural_chunks, full_text_chunks = [], [], []\n",
    "            failed_urls = []\n",
    "    \n",
    "            async def process_with_semaphore(url: str, depth: int) -> Tuple[List[Document], List[Document], List[Document]]:\n",
    "                async with semaphore:\n",
    "                    char_docs, struct_docs, full_docs = await self.process_url(url, depth)\n",
    "                    self.progress_bar.update(1)\n",
    "                    return char_docs, struct_docs, full_docs\n",
    "    \n",
    "            \n",
    "            batch_size = batch_size or min(30, self.max_workers * 2)\n",
    "\n",
    "    \n",
    "            for i in range(0, len(unique_urls_to_process), batch_size):\n",
    "                batch = unique_urls_to_process[i:i+batch_size]\n",
    "    \n",
    "                \n",
    "                #process = psutil.Process()\n",
    "                #logger.info(f\"[Memory Before Batch {i//batch_size + 1}] RSS: {process.memory_info().rss / (1024 ** 2):.2f} MB\")\n",
    "                #logger.info(f\"Processing batch {i//batch_size + 1}/{(len(unique_urls_to_process)-1)//batch_size + 1}\")\n",
    "    \n",
    "                tasks = [process_with_semaphore(url, depth) for url, depth in batch]\n",
    "                results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "                #logger.info(f\"[Memory After Batch {i//batch_size + 1}] RSS: {process.memory_info().rss / (1024 ** 2):.2f} MB\")\n",
    "    \n",
    "                for j, result in enumerate(results):\n",
    "                    url, depth = batch[j]\n",
    "                    if isinstance(result, BaseException):\n",
    "                        self.error_urls[url] = str(result)\n",
    "                        failed_urls.append((url, depth))\n",
    "                        continue\n",
    "    \n",
    "                    char_docs, struct_docs, full_docs = result\n",
    "                    total_chunks = len(char_docs) + len(struct_docs) + len(full_docs)\n",
    "    \n",
    "                    if total_chunks > 0:\n",
    "                        self.processed_urls.add(url)\n",
    "                        if url in self.redirected_urls:\n",
    "                            self.processed_urls.add(self.redirected_urls[url])\n",
    "                    else:\n",
    "                    \n",
    "                        if (\n",
    "                            url not in self.redirected_urls and\n",
    "                            url not in self.error_urls  # ✅ Suppress warning for known bad pages\n",
    "                        ):\n",
    "                            logger.critical(f\"⚠️ No chunks extracted for {url} (processed but empty)\")\n",
    "\n",
    "\n",
    "                    character_chunks.extend(char_docs)\n",
    "                    structural_chunks.extend(struct_docs)\n",
    "                    full_text_chunks.extend(full_docs)\n",
    "    \n",
    "            if failed_urls:\n",
    "                logger.info(f\"Retrying {len(failed_urls)} failed URLs\")\n",
    "                retry_semaphore = asyncio.Semaphore(max(1, self.max_workers // 2))\n",
    "    \n",
    "                async def retry_process(url: str, depth: int):\n",
    "                    async with retry_semaphore:\n",
    "                        await asyncio.sleep(2)\n",
    "                        return await self.process_url(url, depth)\n",
    "    \n",
    "                retry_batch_size = max(1, self.max_workers // 2)\n",
    "                for i in range(0, len(failed_urls), retry_batch_size):\n",
    "                    retry_batch = failed_urls[i:i+retry_batch_size]\n",
    "                    retry_tasks = [retry_process(url, depth) for url, depth in retry_batch]\n",
    "                    retry_results = await asyncio.gather(*retry_tasks, return_exceptions=True)\n",
    "    \n",
    "                    for j, result in enumerate(retry_results):\n",
    "                        url, depth = retry_batch[j]\n",
    "                        if isinstance(result, BaseException):\n",
    "                            continue\n",
    "    \n",
    "                        char_docs, struct_docs, full_docs = result\n",
    "                        total_chunks = len(char_docs) + len(struct_docs) + len(full_docs)\n",
    "    \n",
    "                        if total_chunks > 0:\n",
    "                            self.processed_urls.add(url)\n",
    "                            if url in self.redirected_urls:\n",
    "                                self.processed_urls.add(self.redirected_urls[url])\n",
    "\n",
    "                        else:\n",
    "                            if (\n",
    "                                url not in self.redirected_urls and\n",
    "                                url not in self.error_urls\n",
    "                            ):\n",
    "                                logger.warning(f\"⚠️ After retry: No chunks extracted for {url}\")\n",
    "\n",
    "        \n",
    "    \n",
    "                        character_chunks.extend(char_docs)\n",
    "                        structural_chunks.extend(struct_docs)\n",
    "                        full_text_chunks.extend(full_docs)\n",
    "    \n",
    "            self.all_documents = character_chunks\n",
    "            self.structural_documents = structural_chunks\n",
    "            self.full_text_documents = full_text_chunks\n",
    "\n",
    "            #Sara\n",
    "            # self._save_progress(character_chunks, self.processed_urls, self.error_urls, final=True, chunk_type=\"character\")\n",
    "            # self._save_progress(structural_chunks, self.processed_urls, self.error_urls, final=True, chunk_type=\"structural\")\n",
    "            # self._save_progress(full_text_chunks, self.processed_urls, self.error_urls, final=True, chunk_type=\"full_text\")\n",
    "            self.save_character_counts(final=True)\n",
    "    \n",
    "            logger.info(f\"Processing complete: {len(self.processed_urls)}/{len(unique_urls_to_process)} URLs, {len(self.error_urls)} errors\")\n",
    "            logger.info(f\"Documents: {len(character_chunks)} character, {len(structural_chunks)} structural, {len(full_text_chunks)} full-text\")\n",
    "    \n",
    "            # ✅ Save redirected URLs to file\n",
    "            if self.redirected_urls:\n",
    "                with open(\"redirected_urls.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(self.redirected_urls, f, ensure_ascii=False, indent=2)\n",
    "                logger.info(f\"Saved {len(self.redirected_urls)} redirected URLs to redirected_urls.json\")\n",
    "    \n",
    "            return {\n",
    "                'character_chunks': character_chunks,\n",
    "                'structural_chunks': structural_chunks,\n",
    "                'full_text_chunks': full_text_chunks\n",
    "            }\n",
    "    \n",
    "        finally:\n",
    "            await self.close_session()\n",
    "            self.session = None\n",
    "            if self.progress_bar:\n",
    "                self.progress_bar.close()\n",
    "            # Clean up Playwright browser and context\n",
    "            if self.context:\n",
    "                await self.context.close()\n",
    "                self.context = None\n",
    "            if self.browser:\n",
    "                await self.browser.close()\n",
    "                self.browser = None\n",
    "            if hasattr(self, \"playwright\"):\n",
    "                await self.playwright.stop()\n",
    "        \n",
    "# Add these two functions to your DESYContentProcessor class:\n",
    "\n",
    "    def track_extraction_results(self, url, character_success, structural_success, character_count=0, structural_count=0, error_msg=None):\n",
    "        \"\"\"Track which extraction methods succeeded for each URL\"\"\"\n",
    "        if not hasattr(self, 'extraction_log'):\n",
    "            self.extraction_log = {}\n",
    "        \n",
    "        self.extraction_log[url] = {\n",
    "            'character_chunks_success': character_success,\n",
    "            'structural_chunks_success': structural_success,\n",
    "            'character_chunks_count': character_count,\n",
    "            'structural_chunks_count': structural_count,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'error_message': error_msg,\n",
    "        }\n",
    "        \n",
    "        # Log extraction results\n",
    "        logger.info(f\"URL: {url} | Character: {'✓' if character_success else '✗'} ({character_count}) | Structural: {'✓' if structural_success else '✗'} ({structural_count})\")\n",
    "        if error_msg:\n",
    "            logger.warning(f\"Error for {url}: {error_msg}\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    def print_extraction_summary(self):\n",
    "        \"\"\"Print a simple summary of extraction results\"\"\"\n",
    "        if not hasattr(self, 'extraction_log'):\n",
    "            print(\"No extraction log available\")\n",
    "            return\n",
    "        \n",
    "        total_urls = len(self.extraction_log)\n",
    "        character_successes = sum(1 for log in self.extraction_log.values() if log['character_chunks_success'])\n",
    "        structural_successes = sum(1 for log in self.extraction_log.values() if log['structural_chunks_success'])\n",
    "        both_failed = sum(1 for log in self.extraction_log.values() \n",
    "                         if not log['character_chunks_success'] and not log['structural_chunks_success'])\n",
    "        \n",
    "        print(f\"\\n--- EXTRACTION SUMMARY ---\")\n",
    "        print(f\"Total URLs: {total_urls}\")\n",
    "        print(f\"Character method succeeded: {character_successes}/{total_urls}\")\n",
    "        print(f\"Structural method succeeded: {structural_successes}/{total_urls}\")\n",
    "        print(f\"Both methods failed: {both_failed}\")\n",
    "        print(f\"Character-only failures: {total_urls - character_successes}\")\n",
    "        print(f\"Structural-only failures: {total_urls - structural_successes}\") \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f375c0-1362-4d4f-a92f-5977763b08de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_urls_from_mapping_file(url_map_file):\n",
    "    import json\n",
    "    with open(url_map_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return list(data.keys())  # Assumes mapping is {url: metadata}\n",
    "\n",
    "def batch_urls(urls, batch_size):\n",
    "    for i in range(0, len(urls), batch_size):\n",
    "        yield urls[i:i + batch_size]\n",
    "\n",
    "\n",
    "def export_merged_results(merged, prefix=\"processor\"):\n",
    "    import json\n",
    "    import orjson\n",
    "    from datetime import datetime\n",
    "\n",
    "    # 📌 Use consistent timestamp everywhere\n",
    "    timestamp = datetime.now()\n",
    "    timestamp_str = timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    iso_time = timestamp.isoformat()\n",
    "\n",
    "    # ✅ Validate merged structure\n",
    "    required_keys = ['character_counts_data', 'structural_chunks', 'full_text_chunks', 'character_chunks', 'processed_urls', 'error_urls', 'url_stats']\n",
    "    for key in required_keys:\n",
    "        if key not in merged or not merged[key]:\n",
    "            logger.warning(f\"Missing or empty data for {key} in merged results\")\n",
    "\n",
    "    # def write_json(filename, data):\n",
    "    #     with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    #         json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def write_json(filename, data):\n",
    "        with open(filename, \"wb\") as f:  # Binary mode for orjson\n",
    "            f.write(orjson.dumps(data))    \n",
    "\n",
    "    def format_text_chunks(docs):\n",
    "        # ✅ Sanity check for misalignment\n",
    "        assert len(docs[\"text_chunks\"]) == len(docs[\"document_metadata\"]), \"Mismatch in chunks and metadata\"\n",
    "\n",
    "        return {\n",
    "            \"timestamp\": iso_time,\n",
    "            \"text_chunks\": [\n",
    "                {\n",
    "                    \"content\": doc,\n",
    "                    \"metadata\": meta\n",
    "                }\n",
    "                for doc, meta in zip(docs[\"text_chunks\"], docs[\"document_metadata\"])\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def format_character_counts(data):\n",
    "        total_pages = sum(len(v) for v in data.values())\n",
    "        total_characters = sum(item[\"character_count\"] for v in data.values() for item in v)\n",
    "        total_words = sum(item[\"metadata\"].get(\"word_count\", 0) for v in data.values() for item in v)\n",
    "\n",
    "        return {\n",
    "            \"timestamp\": iso_time,\n",
    "            \"summary\": {\n",
    "                \"total_pages\": total_pages,\n",
    "                \"total_characters\": total_characters,\n",
    "                \"total_words\": total_words,\n",
    "                \"average_characters_per_page\": round(total_characters / total_pages, 2) if total_pages else 0\n",
    "            },\n",
    "            \"pages\": [item for v in data.values() for item in v]\n",
    "        }\n",
    "\n",
    "    write_json(f\"page_character_counts_final.json\", format_character_counts(merged[\"character_counts_data\"])) #_{timestamp_str}\n",
    "\n",
    "    # 🔹 Structural\n",
    "    write_json(f\"{prefix}_structural_base_text_chunks_final.json\", format_text_chunks(merged[\"structural_chunks\"]))\n",
    "    write_json(f\"{prefix}_structural_base_results_final.json\", merged[\"structural_chunks\"][\"document_metadata\"])\n",
    "\n",
    "    # 🔹 Full-text\n",
    "    write_json(f\"{prefix}_sized_base_text_chunks_final.json\", format_text_chunks(merged[\"full_text_chunks\"]))\n",
    "    write_json(f\"{prefix}_sized_base_results_final.json\", merged[\"full_text_chunks\"][\"document_metadata\"])\n",
    "\n",
    "    # 🔹 Character-based\n",
    "    write_json(f\"{prefix}_character_base_text_chunks_final.json\", format_text_chunks(merged[\"character_chunks\"]))\n",
    "    write_json(f\"{prefix}_character_base_results_final.json\", merged[\"character_chunks\"][\"document_metadata\"])\n",
    "\n",
    "    # 🔹 URL tracking\n",
    "    write_json(f\"{prefix}_processed_urls_final.json\", list(merged[\"processed_urls\"]))\n",
    "    write_json(f\"{prefix}_error_urls_final.json\", list(merged[\"error_urls\"]))\n",
    "\n",
    "    # 🔹 Serialize safe url_stats (handles sets, defaultdicts)\n",
    "    safe_url_stats = {\n",
    "        k: list(v) if isinstance(v, set) else v\n",
    "        for k, v in dict(merged[\"url_stats\"]).items()\n",
    "    }\n",
    "    write_json(f\"{prefix}_url_stats_final.json\", safe_url_stats)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "def merge_batch_results(all_results):\n",
    "    merged = {\n",
    "        'character_chunks': {'text_chunks': [], 'document_metadata': []},\n",
    "        'structural_chunks': {'text_chunks': [], 'document_metadata': []},\n",
    "        'full_text_chunks': {'text_chunks': [], 'document_metadata': []},\n",
    "        'character_counts_data': {\n",
    "            'character_chunks': [], 'structural_chunks': [], 'full_text_chunks': []\n",
    "        },\n",
    "        'processed_urls': set(),\n",
    "        'error_urls': set(),\n",
    "        'url_stats': defaultdict(list),\n",
    "    }\n",
    "\n",
    "    for key in ['character_chunks', 'structural_chunks', 'full_text_chunks']:\n",
    "        merged[key]['text_chunks'] = list(chain.from_iterable(r[key]['text_chunks'] for r in all_results))\n",
    "        merged[key]['document_metadata'] = list(chain.from_iterable(r[key]['document_metadata'] for r in all_results))\n",
    "        merged['character_counts_data'][key] = list(chain.from_iterable(r['character_counts_data'][key] for r in all_results))\n",
    "\n",
    "    merged['processed_urls'].update(chain.from_iterable(r['processed_urls'] for r in all_results))\n",
    "    merged['error_urls'].update(chain.from_iterable(r['error_urls'] for r in all_results))\n",
    "\n",
    "    for result in all_results:\n",
    "        for stat_key, stat_val in result['url_stats'].items():\n",
    "            if isinstance(stat_val, (list, set)):\n",
    "                merged['url_stats'][stat_key].extend(stat_val)\n",
    "            elif isinstance(stat_val, dict):\n",
    "                merged['url_stats'][stat_key].append(stat_val)  # Store as list of dicts\n",
    "            elif isinstance(stat_val, int):\n",
    "                merged['url_stats'][stat_key].append(stat_val)  # Accumulate in list\n",
    "\n",
    "    merged['processed_urls'] = list(merged['processed_urls'])\n",
    "    merged['error_urls'] = list(merged['error_urls'])\n",
    "\n",
    "    return merged\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def process_mapped_urls(url_map_file, max_depth, batch_size, limit=None):    \n",
    "    \"\"\"Process URLs from a mapping file up to the specified depth.\"\"\"\n",
    "    \n",
    "    async def _run():\n",
    "        all_urls = load_urls_from_mapping_file(url_map_file)\n",
    "        if limit:\n",
    "            all_urls = all_urls[:limit]    \n",
    "        all_results = []\n",
    "\n",
    "        for batch_num, url_batch in enumerate(batch_urls(all_urls, batch_size), 1):\n",
    "            print(f\"\\n🔹 Processing batch {batch_num} with {len(url_batch)} URLs...\")\n",
    "      \n",
    "            processor = DESYContentProcessor(\n",
    "                max_depth=max_depth,\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=75,              \n",
    "            )\n",
    "\n",
    "            try:\n",
    "                results = await processor.process_urls_from_mapping(\n",
    "                    url_map_file, batch_size, limit=limit\n",
    "                )\n",
    "            except BaseException as e:\n",
    "                # 🔧 CHANGED: Avoid halting the entire pipeline; continue to next batch\n",
    "                logger.warning(f\"Batch {batch_num} failed: {type(e).__name__} - {e}\")\n",
    "                continue\n",
    "\n",
    "            # Track URLs\n",
    "            character_urls = set(doc.metadata.get('source', '') for doc in results['character_chunks'])\n",
    "            structural_urls = set(doc.metadata.get('source', '') for doc in results['structural_chunks'])\n",
    "            full_text_urls = set(doc.metadata.get('source', '') for doc in results['full_text_chunks'])\n",
    "\n",
    "            # 🔧 CHANGED: helper for DRY character counting\n",
    "            def build_chunk_metadata(docs):\n",
    "                return [\n",
    "                    {\n",
    "                        'url': doc.metadata.get('source', ''),\n",
    "                        'chunk_index': i,\n",
    "                        'character_count': len(doc.page_content),\n",
    "                        'metadata': doc.metadata\n",
    "                    }\n",
    "                    for i, doc in enumerate(docs)\n",
    "                ]\n",
    "\n",
    "            character_counts_data = {\n",
    "                'character_chunks': build_chunk_metadata(results['character_chunks']),\n",
    "                'structural_chunks': build_chunk_metadata(results['structural_chunks']),\n",
    "                'full_text_chunks': build_chunk_metadata(results['full_text_chunks']),\n",
    "            }\n",
    "\n",
    "            all_processed_urls = character_urls | structural_urls | full_text_urls\n",
    "            for url in all_processed_urls:\n",
    "                processor.track_extraction_results(\n",
    "                    url=url,\n",
    "                    character_success=url in character_urls,\n",
    "                    structural_success=url in structural_urls,\n",
    "                    character_count=sum(1 for doc in results['character_chunks'] if doc.metadata.get('source') == url),\n",
    "                    structural_count=sum(1 for doc in results['structural_chunks'] if doc.metadata.get('source') == url),\n",
    "                )\n",
    "\n",
    "            missing_in_structural = character_urls - structural_urls\n",
    "            missing_in_character = structural_urls - character_urls\n",
    "\n",
    "            batch_result = {\n",
    "                'character_chunks': {\n",
    "                    'text_chunks': [doc.page_content for doc in results['character_chunks']],\n",
    "                    'document_metadata': [doc.metadata for doc in results['character_chunks']],\n",
    "                },\n",
    "                'structural_chunks': {\n",
    "                    'text_chunks': [doc.page_content for doc in results['structural_chunks']],\n",
    "                    'document_metadata': [doc.metadata for doc in results['structural_chunks']],\n",
    "                },\n",
    "                'full_text_chunks': {\n",
    "                    'text_chunks': [doc.page_content for doc in results['full_text_chunks']],\n",
    "                    'document_metadata': [doc.metadata for doc in results['full_text_chunks']],\n",
    "                },\n",
    "                'character_counts_data': character_counts_data,\n",
    "                'processed_urls': list(processor.processed_urls),\n",
    "                'error_urls': processor.error_urls,\n",
    "                'url_stats': {\n",
    "                    'total_urls_processed': len(processor.processed_urls),\n",
    "                    'total_urls_with_errors': len(processor.error_urls),\n",
    "                    'error_urls': list(processor.error_urls),\n",
    "                    'redirected_urls': processor.redirected_urls,\n",
    "                    'error_url_names': {url: extract_url_name(url) for url in processor.error_urls},\n",
    "                    'total_character_urls': len(character_urls),\n",
    "                    'total_structural_urls': len(structural_urls),\n",
    "                    'total_full_text_urls': len(full_text_urls),\n",
    "                    'missing_in_structural': list(missing_in_structural),\n",
    "                    'missing_in_character': list(missing_in_character),\n",
    "                    'url_names': {url: extract_url_name(url) for url in processor.processed_urls},\n",
    "                    'character_url_names': {url: extract_url_name(url) for url in character_urls},\n",
    "                    'structural_url_names': {url: extract_url_name(url) for url in structural_urls},\n",
    "                    'full_text_url_names': {url: extract_url_name(url) for url in full_text_urls},\n",
    "                },\n",
    "            }\n",
    "\n",
    "            all_results.append(batch_result)\n",
    "\n",
    "            # 🔧 CHANGED: optional anti-bot delay\n",
    "            #await asyncio.sleep(random.uniform(2, 6))\n",
    "\n",
    "        final_result = merge_batch_results(all_results)\n",
    "        return final_result\n",
    "        \n",
    "    return asyncio.run(_run())\n",
    "\n",
    "\n",
    "def extract_url_name(url):\n",
    "    \"\"\"Extract a readable name from a URL\"\"\"\n",
    "    from urllib.parse import urlparse\n",
    "    \n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        # Remove www. if present and get domain\n",
    "        domain = parsed.netloc.replace('www.', '')\n",
    "        \n",
    "        # Get the path without trailing slash\n",
    "        path = parsed.path.rstrip('/')\n",
    "        \n",
    "        # Split path into segments\n",
    "        segments = [seg for seg in path.split('/') if seg]\n",
    "        \n",
    "        if not segments:\n",
    "            return domain  # Just return domain if no path segments\n",
    "            \n",
    "        # Use the last segment as the name (usually most specific)\n",
    "        name = segments[-1].replace('-', ' ').replace('_', ' ')\n",
    "        \n",
    "        # Clean up the name\n",
    "        name = ' '.join(word.capitalize() for word in name.split())\n",
    "        \n",
    "        return f\"{domain} - {name}\" if name else domain\n",
    "    except:\n",
    "        return url  # Return original URL if parsing fails\n",
    "\n",
    "\n",
    "def process_mapped_urls_safe(url_map_file, max_depth, batch_size, limit=None):    \n",
    "    \"\"\"Enhanced synchronous wrapper with better error handling\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    try:        \n",
    "        result = process_mapped_urls(url_map_file, max_depth, batch_size, limit=limit)\n",
    "        if isinstance(result, BaseException):\n",
    "            logger.error(f\"process_mapped_urls failed: {type(result).__name__} - {result}\")\n",
    "            return None\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Processing completed in: {duration}\")\n",
    "        \n",
    "        # Extract URL statistics for reporting\n",
    "        #url_stats = result['url_stats']\n",
    "        url_stats = result.get('url_stats', {}) # to prevent crashes if keys are missing\n",
    "                \n",
    "       \n",
    "        #Chunk summaries\n",
    "        print(f\"🧩 Total character-based chunks: {len(result['character_chunks']['text_chunks'])}\")\n",
    "        print(f\"🔧 Total structure-based chunks: {len(result['structural_chunks']['text_chunks'])}\")\n",
    "        print(f\"📄 Total full-text documents: {len(result['full_text_chunks']['text_chunks'])}\")\n",
    "        print(f\"🔗 Total processed URLs: {url_stats.get('total_urls_processed', 0)}\")\n",
    "        print(f\"❌ Total URLs with errors: {len(result.get('error_urls', []))}\")\n",
    "        \n",
    "        # Report on URL distribution\n",
    "        print(f\"\\n📊 URL Distribution:\")\n",
    "        print(f\"Character chunks: {url_stats.get('total_character_urls', 0)}\")\n",
    "        print(f\"Structural chunks: {url_stats.get('total_structural_urls', 0)}\")\n",
    "        print(f\"Full-text chunks: {url_stats.get('total_full_text_urls', 0)}\")\n",
    "        \n",
    "        # Report on missing URLs\n",
    "        print(f\"\\nMissing URLs Analysis:\")\n",
    "        missing_in_structural = url_stats.get('missing_in_structural', [])\n",
    "        missing_in_character = url_stats.get('missing_in_character', [])\n",
    "\n",
    "        print(f\"\\n🕳️ Missing URLs Analysis:\")\n",
    "        print(f\"Missing in structural: {len(missing_in_structural)}\")\n",
    "        print(f\"Missing in character: {len(missing_in_character)}\")\n",
    "        \n",
    "\n",
    "        # 🔧 CHANGED: safer logging of failed URLs\n",
    "        if missing_in_character:\n",
    "            logger.info(f\"⚠️ URLs failing character-based chunking: {len(missing_in_character)}\")\n",
    "            for url in missing_in_character:\n",
    "                log_entry = result.get('processor', {}).extraction_log.get(url, {}) if 'processor' in result else {}\n",
    "                logger.info(\n",
    "                    f\"Failed URL: {url}, \"\n",
    "                    f\"Content Length: {log_entry.get('content_length', 0)}, \"\n",
    "                    f\"Error: {log_entry.get('error_message', 'Unknown')}\"\n",
    "                )\n",
    "\n",
    "        # 🔧 CHANGED: avoid printing if processor object was removed\n",
    "        processor_obj = result.get('processor')\n",
    "        if processor_obj and hasattr(processor_obj, 'print_extraction_summary'):\n",
    "            processor_obj.print_extraction_summary()\n",
    "\n",
    "        print(f\"✅ Finished processing file: {url_map_file}\")\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        # 🔧 CHANGED: log full traceback for better debugging\n",
    "        import traceback\n",
    "        logging.error(f\"🔥 Critical error in processor: {e}\", exc_info=True)\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_character_counts_json(character_counts_data, export_prefix=\"desy_final\"):\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    timestamp_obj = datetime.now()\n",
    "\n",
    "    timestamp_str = timestamp_obj.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{export_prefix}_character_counts_{timestamp_str}.json\"    \n",
    "    \n",
    "\n",
    "    full_text_data = character_counts_data.get(\"full_text_chunks\", [])\n",
    "    total_characters = sum(item.get(\"character_count\", 0) for item in full_text_data)\n",
    "\n",
    "    # ✅ Optional: wrap with summary and timestamp like your original format\n",
    "    data = {\n",
    "        \"timestamp\": timestamp_obj.isoformat(),\n",
    "        \"pages\": full_text_data,\n",
    "        \"summary\": {\n",
    "            \"total_pages\": len(full_text_data),\n",
    "            \"total_characters\": total_characters,\n",
    "            \"average_characters_per_page\": round(total_characters / len(full_text_data), 2) if full_text_data else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Full-text character counts saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save full-text character counts: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def save_url_stats(url_stats, export_prefix=\"desy_final\"):\n",
    "    import json\n",
    "    import orjson\n",
    "    from datetime import datetime\n",
    "    \n",
    "\n",
    "    # 🔧 CHANGED: Use consistent timestamp object for filenames and metadata\n",
    "    # timestamp_obj = datetime.now()\n",
    "    # timestamp_str = timestamp_obj.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    url_stats = result.get(\"url_stats\", {})  # 🔧 CHANGED: Safer access to avoid KeyError\n",
    "    url_names = url_stats.get(\"url_names\", {})\n",
    "    missing_structural = url_stats.get(\"missing_in_structural\", [])\n",
    "    missing_character = url_stats.get(\"missing_in_character\", [])\n",
    "\n",
    "    # 🔧 CHANGED: Convert sets/defaultdict to plain dict for JSON serialization\n",
    "    safe_url_stats = {\n",
    "        k: list(v) if isinstance(v, set) else v\n",
    "        for k, v in dict(url_stats).items()\n",
    "    }\n",
    "\n",
    "    stats_file = f\"{export_prefix}_url_stats.json\" #_{timestamp_str}\n",
    "    # with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(safe_url_stats, f, indent=2, ensure_ascii=False)\n",
    "    with open(stats_file, 'wb') as f:\n",
    "        f.write(orjson.dumps(safe_url_stats))\n",
    "\n",
    "    # Missing structural URLs\n",
    "    missing_structural_file = f\"{export_prefix}_missing_structural.txt\" #_{timestamp_str}\n",
    "    with open(missing_structural_file, 'w', encoding='utf-8') as f:\n",
    "        for url in missing_structural:\n",
    "            name = url_names.get(url, '')\n",
    "            f.write(f\"{url}\\t{name}\\n\")\n",
    "\n",
    "    # Missing character URLs\n",
    "    missing_character_file = f\"{export_prefix}_missing_character.txt\" #_{timestamp_str}\n",
    "    with open(missing_character_file, 'w', encoding='utf-8') as f:\n",
    "        for url in missing_character:\n",
    "            name = url_names.get(url, '')\n",
    "            f.write(f\"{url}\\t{name}\\n\")\n",
    "\n",
    "    print(f\"\\n📊 URL statistics saved to: {stats_file}\")\n",
    "    print(f\"📄 Missing structural URLs saved to: {missing_structural_file}\")\n",
    "    print(f\"📄 Missing character URLs saved to: {missing_character_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage \n",
    "# Find the most recent URL map file or specify it directly\n",
    "\n",
    "import sys\n",
    "\n",
    "files_to_scrape = [\n",
    "    \"Zero_text_scraped_urls.json\",\n",
    "    \"desy_url_map_20250425_155033_urls=200_000.json\"\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "seen_urls = set()\n",
    "sys.stdout = open('output_log.txt', 'w', encoding='utf-8') # Redirect stdout to a file\n",
    "\n",
    "\n",
    "    \n",
    "def map_urls_to_depth(url_map):\n",
    "    depth_dict = {}\n",
    "    for depth_key, url_list in url_map.get(\"urls_by_depth\", {}).items():\n",
    "        try:\n",
    "            depth_int = int(depth_key)\n",
    "        except (ValueError, TypeError):\n",
    "            print(f\"⚠️ Skipping non-numeric depth key: {depth_key}\")\n",
    "            continue\n",
    "        for url in url_list:\n",
    "            if url not in depth_dict or depth_int < depth_dict[url]:\n",
    "                depth_dict[url] = depth_int\n",
    "    return depth_dict\n",
    "\n",
    "\n",
    "\n",
    "for map_file in files_to_scrape:\n",
    "    try:\n",
    "        print(f\"🧭 Scraping from file: {map_file}\")\n",
    "\n",
    "        # Load URLs from mapping file before scraping\n",
    "        with open(map_file, 'r', encoding='utf-8') as f:\n",
    "            url_map = json.load(f)\n",
    "            url_depth_map = map_urls_to_depth(url_map)\n",
    "            new_urls = []\n",
    "            #for depth_values in url_map.get(\"urls_by_depth\", {}).values():\n",
    "            for url, depth in url_depth_map.items():    \n",
    "                #for URL, depth in url_depth_map:\n",
    "                #for URL, depth in url_depth_map.items():\n",
    "                if url not in seen_urls:\n",
    "                    seen_urls.add(url)\n",
    "                    new_urls.append(url)\n",
    "\n",
    "        # Skip if no new URLs to scrape\n",
    "        if not new_urls:\n",
    "            print(f\"⚠️ All URLs in {map_file} are already scraped — skipping.\")\n",
    "            continue\n",
    "\n",
    "        result = process_mapped_urls_safe(map_file, max_depth=2, batch_size=100, limit=1000)\n",
    "        if result:\n",
    "            all_results.append(result)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ {map_file} not found. Please run the URL mapper to generate it.\")\n",
    "\n",
    "\n",
    "# Restore stdout\n",
    "sys.stdout.close()\n",
    "sys.stdout = sys.__stdout__\n",
    "\n",
    "# Merge and export all results together\n",
    "merged = merge_batch_results(all_results)\n",
    "export_merged_results(merged, prefix=\"desy_final\")\n",
    "print(f\"🔍 Type of merged: {type(merged)}\")\n",
    "print(f\"🔍 Keys in merged (if dict): {list(merged.keys()) if isinstance(merged, dict) else 'Not a dict'}\")\n",
    "\n",
    "save_url_stats(merged[\"url_stats\"], export_prefix=\"desy_final\")\n",
    "#save_url_stats(merged, export_prefix=\"desy_final\")\n",
    "save_character_counts_json(merged['character_counts_data'], export_prefix=\"desy_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddc1b9-92cd-4ad8-8129-720298524f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968fb2e-a210-4eaf-91a1-9af79fdf3796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9dac4-ba72-4d84-ac4e-bef939e3bdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885334a-eb23-4f1f-b646-7a32f93caa64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83951d6f-b3a6-4300-8ea1-3c1e9fa382d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5089bf9a-0b95-4e46-9ad0-1d8d223f5c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca5faa8-c42d-4675-872e-30fa82ea346c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f0bfa-dc43-46e7-9510-0e0986b63ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
